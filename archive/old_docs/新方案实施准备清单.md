# 新方案实施准备清单

## 当前状态（原计划）

### 进度快照
```
✅ CUSTOM:    800/800 (100%)
🔄 DEEPSEEK:  40/800 (5%)
⏳ QWEN:      0/800 (0%)
━━━━━━━━━━━━━━━━━━━━━━━━
总计: 840/2400 (35%)
预计完成时间: ~4.1小时后
```

### 原计划数据特点
- **模型**: custom (gpt-4o-mini)
- **话题**: 39个（重复率高）
- **格式**: 86.7% Markdown
- **质量**: 平均0.90（过于一致）
- **规模**: 2400条（3模型×800）

---

## 新方案目标（改进计划）

### 数据集构成
```
AI样本:   25,000条 (50%)
  ├─ custom:   10,000条
  ├─ deepseek:  8,000条
  └─ qwen:      7,000条

人类样本: 25,000条 (50%)
  ├─ CCI 3.0-HQ:    8,000条
  ├─ 搜狐新闻:       6,000条
  ├─ 知乎KOL:        5,000条
  ├─ 百度知道:       3,000条
  ├─ THUCNews:       2,000条
  └─ 微博:           1,000条
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
总计: 50,000条
```

---

## 实施步骤（原计划完成后）

### 阶段一：改进AI样本生成（3-5天）

#### Step 1: 修改代码配置
**文件**: `new data collection.py`

**修改1: 多模型配置（第765行）**
```python
# 改为
SAMPLES_PER_MODEL = {
    "custom": 10000,
    "deepseek": 8000,
    "qwen": 7000
}
```

**修改2: 增加话题数量（第433行）**
```python
# 改为
num_topics=200  # 从50增加到200
```

**修改3: 格式变异（第597行后添加）**
```python
# 在 _generate_prompt 方法中添加格式随机化
format_instruction = random.choices(
    ["请使用Markdown格式组织内容",
     "请使用纯文本格式，不要使用Markdown标记",
     ""],
    weights=[0.3, 0.3, 0.4]
)[0]
```

#### Step 2: 清除旧缓存
```bash
rm -rf dimension_cache/*
rm auto_generated_datasets/auto_generation_plan.json
```

#### Step 3: 启动新生成
```bash
echo "2" | ./venv/bin/python "new data collection.py"
```

**预计耗时**:
- Custom 10000条: ~26小时
- DeepSeek 8000条: ~21小时
- Qwen 7000条: ~18小时
- **总计**: ~65小时（约3天）

---

### 阶段二：获取人类样本（1-2周）

#### Step 1: 下载公开数据集

**CCI 3.0-HQ (8000条)**
```bash
# 下载地址
wget http://open.flopsera.com/flopsera-open/details/BAAI-CCI2

# 或使用HuggingFace
huggingface-cli download BAAI/CCI3-HQ
```

**搜狐新闻 (6000条)**
```bash
# 访问: https://dianshudata.com/dataDetail/11991
# 需要注册账号下载
```

**知乎KOL (5000条)**
```bash
huggingface-cli download wangrui6/Zhihu-KOL
```

#### Step 2: 创建预处理脚本

**新建文件**: `human_data_processor.py`

核心功能：
- 文本清洗（去HTML、URL、特殊符号）
- 质量过滤（长度100-2000字）
- 分布匹配（使长度/领域匹配AI样本）

#### Step 3: 数据采样与清洗
```bash
./venv/bin/python human_data_processor.py --source cci --output human_samples/cci_8000.csv --target 8000
./venv/bin/python human_data_processor.py --source souhu --output human_samples/souhu_6000.csv --target 6000
./venv/bin/python human_data_processor.py --source zhihu --output human_samples/zhihu_5000.csv --target 5000
```

---

### 阶段三：数据集合并（3-5天）

#### Step 1: 合并AI样本
```python
import pandas as pd

ai_files = [
    'auto_generated_datasets/auto_dataset_custom_*.csv',
    'auto_generated_datasets/auto_dataset_deepseek_*.csv',
    'auto_generated_datasets/auto_dataset_qwen_*.csv'
]

ai_df = pd.concat([pd.read_csv(f, encoding='utf-8-sig') for f in ai_files])
ai_df['label'] = 1  # AI标签
```

#### Step 2: 合并人类样本
```python
human_files = [
    'human_samples/cci_8000.csv',
    'human_samples/souhu_6000.csv',
    'human_samples/zhihu_5000.csv',
    # ...
]

human_df = pd.concat([pd.read_csv(f, encoding='utf-8-sig') for f in human_files])
human_df['label'] = 0  # 人类标签
```

#### Step 3: 统一格式并合并
```python
final_df = pd.concat([ai_df, human_df], ignore_index=True)
final_df = final_df.sample(frac=1, random_state=42)  # 打乱顺序

# 划分数据集
train_df = final_df[:35000]  # 70%
val_df = final_df[35000:42500]  # 15%
test_df = final_df[42500:]  # 15%
```

#### Step 4: 质量验证
- 分布一致性检查（KS检验）
- 长度分布对比
- 领域覆盖验证
- 样本质量抽查

---

### 阶段四：基线模型验证（1周）

#### 训练简单分类器
```python
from transformers import BertForSequenceClassification, Trainer

model = BertForSequenceClassification.from_pretrained('bert-base-chinese')
# 训练基线模型
# 目标准确率 > 80%
```

---

## 检查清单（原计划完成后执行）

### 准备工作
- [ ] 原计划2400条数据已完成
- [ ] 代码已备份（new data collection.py.backup_*）
- [ ] 改进计划已保存
- [ ] 虚拟环境已准备

### 代码修改
- [ ] 第765行：修改SAMPLES_PER_MODEL
- [ ] 第433行：话题数量改为200
- [ ] 第597行：添加格式变异逻辑
- [ ] 清除旧缓存文件

### 数据准备
- [ ] 下载CCI 3.0-HQ数据集
- [ ] 下载搜狐新闻数据
- [ ] 下载知乎KOL数据集
- [ ] 创建human_data_processor.py

### 执行生成
- [ ] 启动新的AI样本生成（25000条）
- [ ] 监控生成进度
- [ ] 处理人类样本（25000条）
- [ ] 合并最终数据集（50000条）

### 验证测试
- [ ] 分布一致性检查
- [ ] 质量评分验证
- [ ] 基线模型训练
- [ ] 准确率达标（>80%）

---

## 监控命令

### 查看当前进度
```bash
./venv/bin/python monitor_progress.py
```

### 持续监控（每5分钟刷新）
```bash
./venv/bin/python monitor_progress.py --watch
```

### 查看后台进程
```bash
ps aux | grep "new data collection"
```

### 查看实时日志
```bash
tail -f dataset_generation.log
```

---

## 时间线估算

```
现在: 2026-01-06 16:30
│
├─ 原计划完成: 2026-01-06 20:30 (4小时后)
│  └─ 输出: 2400条AI样本（custom 800 + deepseek 800 + qwen 800）
│
├─ 新方案启动: 2026-01-06 21:00
│
├─ AI样本完成: 2026-01-09 14:00 (3天后)
│  └─ 输出: 25000条AI样本（custom 10k + deepseek 8k + qwen 7k）
│
├─ 人类样本完成: 2026-01-13 (1周后)
│  └─ 输出: 25000条人类样本（CCI 8k + 搜狐 6k + 知乎 5k + 其他 6k）
│
├─ 数据集合并: 2026-01-16 (10天后)
│  └─ 输出: 50000条完整数据集（AI 25k + 人类 25k）
│
└─ 验证完成: 2026-01-20 (2周后)
   └─ 基线准确率 > 80%
```

**总耗时预估**: 约2周

---

## 注意事项

### API成本控制
- Custom: ~10k条 × $0.002 ≈ $20
- DeepSeek: ~8k条 × $0.001 ≈ $8
- Qwen: ~7k条 × $0.001 ≈ $7
- **总计**: ~$35

### 存储空间
- AI样本: 25000条 × 1KB ≈ 25MB
- 人类样本: 25000条 × 1KB ≈ 25MB
- **总计**: ~50MB

### 风险应对
1. **API限流**: 控制请求频率（0.5秒/条）
2. **数据下载受限**: 优先使用开源数据集
3. **质量不达标**: 设置质量阈值>0.6，严格筛选

---

## 下一步行动

**等待原计划完成后：**
1. 运行监控脚本确认完成
2. 备份原数据
3. 修改代码配置
4. 清除缓存
5. 启动新方案生成

**当前可以做：**
- [x] 创建监控脚本
- [x] 准备改进计划
- [ ] 下载人类样本数据集（提前准备）
- [ ] 编写human_data_processor.py脚本
