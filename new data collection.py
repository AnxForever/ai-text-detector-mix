import openai
import pandas as pd
import time
import random
import json
import os
from datetime import datetime
from typing import List, Dict, Optional
import logging

# ==================== 1. é…ç½®å¤šæ¨¡å‹APIå®¢æˆ·ç«¯ ====================

MODEL_CONFIGS = {
    "custom": {  # è‡ªå®šä¹‰APIç«¯ç‚¹ (å·²æµ‹è¯•å¯ç”¨)
        "client_class": openai.OpenAI,
        "api_key": "sk-p14OddEwPKmsWMVkBsmckJrKnMQRo8xSlzOhNcYmAtZ5JSbO",
        "base_url": "https://china.184772.xyz/v1",
        "model_name": "gpt-4o-mini"
    },
    "deepseek": {
        "client_class": openai.OpenAI,
        "api_key": "sk-c6d1be1eab4a4981b2efa3110f037376",
        "base_url": "https://api.deepseek.com",
        "model_name": "deepseek-chat"
    },
    "qwen": {  # é€šä¹‰åƒé—® (é€šè¿‡DashScopeå¹³å°)
        "client_class": openai.OpenAI,
        "api_key": "sk-bfceae8516c94a5693e135d54fdc3900",
        "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
        "model_name": "qwen3-max"
    },
    # å¯ä»¥ç»§ç»­æ·»åŠ å…¶ä»–æ¨¡å‹
}

# åˆå§‹åŒ–æ‰€æœ‰å®¢æˆ·ç«¯
clients = {}
for model_name, config in MODEL_CONFIGS.items():
    try:
        clients[model_name] = config["client_class"](
            api_key=config["api_key"],
            base_url=config["base_url"]
        )
        print(f"âœ… {model_name.upper()} å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ {model_name.upper()} å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        clients[model_name] = None

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('dataset_generation.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


# ==================== 2. è‡ªåŠ¨ç»´åº¦ç”Ÿæˆå™¨ ====================

class AutoDimensionGenerator:
    """ä½¿ç”¨LLMè‡ªåŠ¨ç”Ÿæˆæ‰€æœ‰ç»´åº¦çš„å†…å®¹"""

    def __init__(self, client, model="gpt-4o-mini"):
        self.client = client
        self.model = model
        self.cache_dir = "dimension_cache"
        os.makedirs(self.cache_dir, exist_ok=True)

    def generate_with_cache(self, cache_key: str, generation_func, *args, **kwargs):
        """å¸¦ç¼“å­˜çš„ç”Ÿæˆå‡½æ•°"""
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.json")

        if os.path.exists(cache_file):
            with open(cache_file, 'r', encoding='utf-8') as f:
                cached_data = json.load(f)
                if cached_data.get('expires', 0) > time.time():
                    logger.info(f"ä»ç¼“å­˜åŠ è½½: {cache_key}")
                    return cached_data['data']

        # ç”Ÿæˆæ–°æ•°æ®
        data = generation_func(*args, **kwargs)

        # ç¼“å­˜24å°æ—¶
        cache_data = {
            'data': data,
            'expires': time.time() + 24 * 3600,
            'created': datetime.now().isoformat()
        }

        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)

        logger.info(f"ç”Ÿæˆå¹¶ç¼“å­˜: {cache_key}")
        return data

    def generate_topics(self, num_topics=30, categories=None):
        """è‡ªåŠ¨ç”Ÿæˆå¤šæ ·åŒ–çš„è¯é¢˜"""
        if categories is None:
            categories = ["ç§‘æŠ€ä¸åˆ›æ–°", "ç¤¾ä¼šä¸æ°‘ç”Ÿ", "ç»æµä¸å•†ä¸š", "æ•™è‚²ä¸å­¦ä¹ ",
                          "å¥åº·ä¸åŒ»ç–—", "ç¯å¢ƒä¸ç”Ÿæ€", "æ–‡åŒ–ä¸è‰ºæœ¯", "å“²å­¦ä¸æ€è€ƒ"]

        prompt = f"""è¯·ç”Ÿæˆ{num_topics}ä¸ªå…·æœ‰æ·±åº¦è®¨è®ºä»·å€¼çš„å†™ä½œè¯é¢˜ã€‚è¦æ±‚ï¼š

1. è¯é¢˜åº”è¯¥ï¼š
   - å…·æœ‰ç°å®æ„ä¹‰å’Œè®¨è®ºä»·å€¼
   - åŒ…å«ä¸€å®šçš„äº‰è®®æ€§æˆ–å¤šè§†è§’ç©ºé—´
   - é€‚åˆä¸åŒæ•™è‚²èƒŒæ™¯çš„äººç†è§£
   - é¿å…è¿‡äºæŠ€æœ¯åŒ–çš„ä¸“ä¸šæœ¯è¯­
   - æ¯ä¸ªè¯é¢˜éƒ½æ˜¯ä¸€ä¸ªå®Œæ•´çš„å¥å­

2. è¯é¢˜ç±»å‹åˆ†å¸ƒï¼š
   - 30% å½“å‰ç¤¾ä¼šçƒ­ç‚¹é—®é¢˜
   - 30% é•¿æœŸå­˜åœ¨çš„æ ¹æœ¬æ€§é—®é¢˜
   - 20% æœªæ¥è¶‹åŠ¿ä¸é¢„æµ‹æ€§è¯é¢˜
   - 20% è·¨å­¦ç§‘äº¤å‰è¯é¢˜

3. è¦†ç›–ä»¥ä¸‹ç±»åˆ«ï¼š{', '.join(categories)}

è¯·ç›´æ¥è¿”å›è¯é¢˜åˆ—è¡¨ï¼Œæ¯ä¸ªè¯é¢˜ä¸€è¡Œï¼Œä¸è¦ç¼–å·ã€‚
"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.8,
                max_tokens=2000
            )

            content = response.choices[0].message.content.strip()
            # æŒ‰è¡Œåˆ†å‰²å¹¶æ¸…ç†
            topics = [line.strip(' "-') for line in content.split('\n') if line.strip()]
            topics = [t for t in topics if len(t) > 5 and len(t) < 100]  # é•¿åº¦è¿‡æ»¤

            # ç¡®ä¿æ•°é‡
            if len(topics) < num_topics:
                # è¡¥å……ä¸€äº›é€šç”¨è¯é¢˜
                default_topics = [
                    "äººå·¥æ™ºèƒ½å¯¹åˆ›æ„äº§ä¸šçš„å½±å“",
                    "å…¨çƒåŒ–èƒŒæ™¯ä¸‹çš„æ–‡åŒ–è®¤åŒå±æœº",
                    "ç¤¾äº¤åª’ä½“å¯¹é’å°‘å¹´å¿ƒç†å¥åº·çš„å½±å“",
                    "æ°”å€™å˜åŒ–å¯¹å†œä¸šç”Ÿäº§çš„é•¿è¿œå½±å“",
                    "è¿œç¨‹åŠå…¬å¯¹åŸå¸‚å‘å±•çš„æ”¹å˜",
                    "äººå£è€é¾„åŒ–å¯¹ç¤¾ä¼šç¦åˆ©çš„æŒ‘æˆ˜",
                    "æ•™è‚²å…¬å¹³åœ¨æ•°å­—åŒ–æ—¶ä»£çš„å®ç°è·¯å¾„",
                    "æ•°æ®éšç§ä¸å•†ä¸šåˆ›æ–°çš„å¹³è¡¡ä¹‹é“"
                ]
                topics.extend(default_topics[:num_topics - len(topics)])

            return topics[:num_topics]

        except Exception as e:
            logger.error(f"ç”Ÿæˆè¯é¢˜å¤±è´¥: {e}")
            return self._get_default_topics(num_topics)

    def generate_genres(self, num_genres=20):
        """è‡ªåŠ¨ç”Ÿæˆå¤šç§æ–‡ä½“æ ¼å¼"""
        prompt = f"""è¯·ç”Ÿæˆ{num_genres}ç§ä¸åŒçš„å†™ä½œä½“è£å’Œæ ¼å¼ï¼Œè¦æ±‚ï¼š

1. è¦†ç›–å„ç§å®é™…åº”ç”¨åœºæ™¯ï¼š
   - æ­£å¼æ–‡æ¡£ç±»ï¼ˆå¦‚æŠ¥å‘Šã€è®ºæ–‡ç­‰ï¼‰
   - å•†ä¸šåº”ç”¨ç±»ï¼ˆå¦‚é‚®ä»¶ã€ææ¡ˆç­‰ï¼‰
   - åª’ä½“ä¼ æ’­ç±»ï¼ˆå¦‚æ–‡ç« ã€æ¨æ–‡ç­‰ï¼‰
   - ä¸ªäººè¡¨è¾¾ç±»ï¼ˆå¦‚æ—¥è®°ã€åšå®¢ç­‰ï¼‰
   - åˆ›æ„å†™ä½œç±»ï¼ˆå¦‚å°è¯´ã€è¯—æ­Œç­‰ï¼‰

2. æ¯ç§æ ¼å¼åº”æœ‰æ˜ç¡®çš„ç‰¹å¾æè¿°ï¼Œä¾‹å¦‚ï¼š
   "å­¦æœ¯è®ºæ–‡æ‘˜è¦ï¼ˆ300-500å­—ï¼Œéœ€åŒ…å«ç ”ç©¶èƒŒæ™¯ã€æ–¹æ³•ã€ç»“æœã€ç»“è®ºï¼‰"
   "äº§å“å‘å¸ƒä¼šæ¼”è®²ç¨¿ï¼ˆå¯Œæœ‰æ„ŸæŸ“åŠ›ï¼Œçªå‡ºäº§å“äº®ç‚¹å’Œç”¨æˆ·ä»·å€¼ï¼‰"

è¯·ç›´æ¥è¿”å›æ ¼å¼åˆ—è¡¨ï¼Œæ¯ä¸ªæ ¼å¼ä¸€è¡Œã€‚
"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=1500
            )

            content = response.choices[0].message.content.strip()
            genres = [line.strip(' "-') for line in content.split('\n') if line.strip()]
            genres = [g for g in genres if len(g) > 3 and len(g) < 80]

            if len(genres) < num_genres:
                default_genres = [
                    "å­¦æœ¯è®ºæ–‡æ‘˜è¦",
                    "æ·±åº¦æ–°é—»æŠ¥é“",
                    "å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ",
                    "çŸ¥ä¹é«˜è´¨é‡å›ç­”",
                    "ä¸ªäººåšå®¢æ–‡ç« ",
                    "å•†ä¸šåˆ†ææŠ¥å‘Š",
                    "äº§å“æµ‹è¯„æŠ¥å‘Š",
                    "æ—…è¡Œæ¸¸è®°",
                    "ä¹¦è¯„å½±è¯„",
                    "æ—¥è®°ç‰‡æ®µ"
                ]
                genres.extend(default_genres[:num_genres - len(genres)])

            return genres[:num_genres]

        except Exception as e:
            logger.error(f"ç”Ÿæˆæ–‡ä½“å¤±è´¥: {e}")
            return self._get_default_genres(num_genres)

    def generate_roles(self, num_roles=15):
        """è‡ªåŠ¨ç”Ÿæˆå¤šæ ·åŒ–çš„è§’è‰²è§†è§’"""
        prompt = f"""è¯·ç”Ÿæˆ{num_roles}ä¸ªä¸åŒçš„å†™ä½œè§’è‰²å’Œè§†è§’ï¼Œè¦æ±‚ï¼š

1. å¤šæ ·åŒ–è¦†ç›–ï¼š
   - ä¸åŒå¹´é¾„é˜¶æ®µï¼ˆå­¦ç”Ÿã€é’å¹´ã€ä¸­å¹´ã€è€å¹´ï¼‰
   - ä¸åŒèŒä¸šèƒŒæ™¯ï¼ˆä¸“ä¸šäººå£«ã€è‡ªç”±èŒä¸šè€…ã€ä¼ä¸šå®¶ç­‰ï¼‰
   - ä¸åŒç«‹åœºæ€åº¦ï¼ˆæ”¯æŒè€…ã€åå¯¹è€…ã€ä¸­ç«‹è€…ã€æ€€ç–‘è€…ç­‰ï¼‰
   - ä¸åŒç”Ÿæ´»ç»å†ï¼ˆäº²å†è€…ã€è§‚å¯Ÿè€…ã€ç ”ç©¶è€…ã€å—å½±å“è€…ç­‰ï¼‰

2. æ¯ä¸ªè§’è‰²åº”æœ‰ç‹¬ç‰¹çš„è§†è§’ç‰¹å¾ï¼Œä¾‹å¦‚ï¼š
   "ä¸€åå…³æ³¨ç§‘æŠ€ä¼¦ç†çš„å“²å­¦å®¶"
   "ç»å†è¿‡æ•°å­—åŒ–è½¬å‹çš„ä¼ ç»Ÿè¡Œä¸šä»ä¸šè€…"
   "å¯¹æ–°æŠ€æœ¯æ—¢æœŸå¾…åˆæ‹…å¿§çš„æ™®é€šæ¶ˆè´¹è€…"

è¯·ç›´æ¥è¿”å›è§’è‰²åˆ—è¡¨ï¼Œæ¯ä¸ªè§’è‰²ä¸€è¡Œã€‚
"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.8,
                max_tokens=1500
            )

            content = response.choices[0].message.content.strip()
            roles = [line.strip(' "-') for line in content.split('\n') if line.strip()]
            roles = [r for r in roles if len(r) > 3 and len(r) < 50]

            if len(roles) < num_roles:
                default_roles = [
                    "ä¸€åé«˜ä¸­ç”Ÿ",
                    "å¤§å­¦ç›¸å…³ä¸“ä¸šæ•™æˆ",
                    "è¡Œä¸šåˆ†æå¸ˆ",
                    "åˆ›ä¸šè€…",
                    "èµ„æ·±è®°è€…",
                    "æ™®é€šæ¶ˆè´¹è€…",
                    "é€€ä¼‘è€äºº",
                    "äº²å†è€…",
                    "æŒæ€€ç–‘æ€åº¦çš„è§‚å¯Ÿè€…",
                    "ä¹è§‚çš„æ”¯æŒè€…"
                ]
                roles.extend(default_roles[:num_roles - len(roles)])

            return roles[:num_roles]

        except Exception as e:
            logger.error(f"ç”Ÿæˆè§’è‰²å¤±è´¥: {e}")
            return self._get_default_roles(num_roles)

    def generate_styles(self, num_styles=10):
        """è‡ªåŠ¨ç”Ÿæˆå†™ä½œé£æ ¼"""
        prompt = f"""è¯·ç”Ÿæˆ{num_styles}ç§ä¸åŒçš„å†™ä½œé£æ ¼æè¿°ï¼Œè¦æ±‚ï¼š

æ¯ç§é£æ ¼åº”åŒ…å«è¯­è¨€ç‰¹ç‚¹å’Œæƒ…æ„ŸåŸºè°ƒï¼Œä¾‹å¦‚ï¼š
"ä¸¥è°¨å­¦æœ¯é£æ ¼ï¼šé€»è¾‘ä¸¥å¯†ï¼Œå¼•ç”¨æ•°æ®ï¼Œå®¢è§‚ä¸­ç«‹"
"çƒ­æƒ…æ´‹æº¢é£æ ¼ï¼šå¯Œæœ‰æ„ŸæŸ“åŠ›ï¼Œä½¿ç”¨ä¿®è¾æ‰‹æ³•ï¼Œæƒ…æ„Ÿå……æ²›"

è¯·ç›´æ¥è¿”å›é£æ ¼åˆ—è¡¨ï¼Œæ¯ä¸ªé£æ ¼ä¸€è¡Œã€‚
"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.9,
                max_tokens=1000
            )

            content = response.choices[0].message.content.strip()
            styles = [line.strip(' "-') for line in content.split('\n') if line.strip()]
            styles = [s for s in styles if len(s) > 5 and len(s) < 50]

            if len(styles) < num_styles:
                default_styles = [
                    "ç®€æ´å®¢è§‚ï¼Œå¹³å®ç›´è¿°",
                    "ä¸¥è°¨å­¦æœ¯ï¼Œå¼•ç”¨æ•°æ®ä¸æ–‡çŒ®",
                    "çƒ­æƒ…æ´‹æº¢ï¼Œå¯Œæœ‰æ„ŸæŸ“åŠ›",
                    "å¹½é»˜é£è¶£ï¼Œå¸¦ç‚¹è°ƒä¾ƒ",
                    "å¨“å¨“é“æ¥ï¼Œå……æ»¡æ•…äº‹æ€§",
                    "æ‰¹åˆ¤æ€§æ€ç»´ï¼Œå¤šè§’åº¦è´¨ç–‘",
                    "è¯—æ„åŒ–è¡¨è¾¾ï¼Œæ³¨é‡æ„è±¡",
                    "å¯¹è¯å¼è¯­æ°”ï¼Œäº²åˆ‡è‡ªç„¶"
                ]
                styles.extend(default_styles[:num_styles - len(styles)])

            return styles[:num_styles]

        except Exception as e:
            logger.error(f"ç”Ÿæˆé£æ ¼å¤±è´¥: {e}")
            return self._get_default_styles(num_styles)

    def generate_constraints(self, num_constraints=8):
        """è‡ªåŠ¨ç”Ÿæˆå†™ä½œçº¦æŸ"""
        prompt = f"""è¯·ç”Ÿæˆ{num_constraints}ä¸ªä¸åŒçš„å†™ä½œè¦æ±‚æˆ–çº¦æŸæ¡ä»¶ï¼Œè¦æ±‚ï¼š

æ¯ä¸ªçº¦æŸåº”å…·ä½“æ˜ç¡®ï¼Œå…·æœ‰å¯æ“ä½œæ€§ï¼Œä¾‹å¦‚ï¼š
"å…¨æ–‡ä¸å°‘äº400å­—"
"è‡³å°‘åŒ…å«ä¸‰ä¸ªåˆ†è®ºç‚¹æˆ–ä¸‰ä¸ªæ ¸å¿ƒç»†èŠ‚"
"å¼•ç”¨ä¸€ä¸ªçœŸå®çš„æ•°æ®ã€ç ”ç©¶æˆ–å†å²äº‹ä»¶"

è¯·ç›´æ¥è¿”å›çº¦æŸåˆ—è¡¨ï¼Œæ¯ä¸ªçº¦æŸä¸€è¡Œã€‚
"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=800
            )

            content = response.choices[0].message.content.strip()
            constraints = [line.strip(' "-') for line in content.split('\n') if line.strip()]
            constraints = [c for c in constraints if len(c) > 5 and len(c) < 60]

            if len(constraints) < num_constraints:
                default_constraints = [
                    "å…¨æ–‡ä¸å°‘äº400å­—",
                    "è‡³å°‘åŒ…å«ä¸‰ä¸ªåˆ†è®ºç‚¹æˆ–ä¸‰ä¸ªæ ¸å¿ƒç»†èŠ‚",
                    "å¼•ç”¨ä¸€ä¸ªçœŸå®çš„æ•°æ®ã€ç ”ç©¶æˆ–å†å²äº‹ä»¶",
                    "é¿å…ä½¿ç”¨ä»»ä½•ä¸“ä¸šæœ¯è¯­ï¼ŒåŠ›æ±‚é€šä¿—æ˜“æ‡‚",
                    "åœ¨ç»“å°¾å¤„æå‡ºä¸€ä¸ªå¼•äººæ·±æ€çš„é—®é¢˜",
                    "ä½¿ç”¨ä¸€ä¸ªç”ŸåŠ¨çš„æ¯”å–»æˆ–ç±»æ¯”æ¥è¯´æ˜æ ¸å¿ƒè§‚ç‚¹",
                    "åŒ…å«æ­£åä¸¤æ–¹é¢çš„è§‚ç‚¹åˆ†æ",
                    "æä¾›å…·ä½“çš„è¡ŒåŠ¨å»ºè®®æˆ–è§£å†³æ–¹æ¡ˆ"
                ]
                constraints.extend(default_constraints[:num_constraints - len(constraints)])

            return constraints[:num_constraints]

        except Exception as e:
            logger.error(f"ç”Ÿæˆçº¦æŸå¤±è´¥: {e}")
            return self._get_default_constraints(num_constraints)

    def _get_default_topics(self, num):
        """é»˜è®¤è¯é¢˜"""
        topics = [
            "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—è¯Šæ–­ä¸­çš„æœ€æ–°çªç ´",
            "é‡å­è®¡ç®—æœºèƒ½å¦ç ´è§£å½“å‰çš„æ‰€æœ‰åŠ å¯†ç®—æ³•",
            "åœ¨çº¿æ•™è‚²å¹³å°å¦‚ä½•ä¿éšœå­¦ç”Ÿçš„å­¦ä¹ æ•ˆæœä¸ä¸“æ³¨åº¦",
            "ä¸€äºŒçº¿åŸå¸‚å¹´è½»äººä¸ºä½•å…´èµ·'åå‘æ¶ˆè´¹'è¶‹åŠ¿",
            "ç¤¾åŒºå…»è€ä¸æœºæ„å…»è€å„è‡ªçš„ä¼˜åŠ¿ä¸æŒ‘æˆ˜",
            "æ­£å¿µå†¥æƒ³å¯¹ç¼“è§£èŒåœºäººç„¦è™‘æƒ…ç»ªçš„æœ‰æ•ˆæ€§ç ”ç©¶",
            "ä»£ç³–é¥®æ–™æ˜¯å¦çœŸçš„æ¯”å«ç³–é¥®æ–™æ›´å¥åº·",
            "æç«¯å¤©æ°”é¢‘å‘ï¼Œä¸ªäººå¦‚ä½•å‚ä¸æ°”å€™å˜åŒ–åº”å¯¹",
            "åŸå¸‚'æµ·ç»µåŒ–'æ”¹é€ å¯¹è§£å†³å†…æ¶é—®é¢˜çš„å®é™…æ•ˆæœ",
            "ç½‘çº¢ç»æµå¯¹ä¼ ç»Ÿæ¶ˆè´¹å“è¥é”€æ¨¡å¼çš„å†²å‡»ä¸å¯ç¤º",
            "å°å¾®ä¼ä¸šå¦‚ä½•åœ¨æ•°å­—ç»æµæ—¶ä»£æ‰¾åˆ°ç”Ÿå­˜ç©ºé—´"
        ]
        return topics[:min(num, len(topics))]

    def _get_default_genres(self, num):
        """é»˜è®¤æ–‡ä½“"""
        genres = [
            "åšå®¢æ–‡ç« ",
            "å­¦æœ¯è®ºæ–‡æ‘˜è¦",
            "äº§å“æµ‹è¯„æŠ¥å‘Š",
            "æ­£å¼çš„å•†ä¸šé‚®ä»¶",
            "å¾®ä¿¡å…¬ä¼—å·æ¨æ–‡",
            "çŸ¥ä¹é—®ç­”",
            "å°å­¦ç”Ÿä½œæ–‡",
            "æ—¥è®°ç‰‡æ®µ"
        ]
        return genres[:min(num, len(genres))]

    def _get_default_roles(self, num):
        """é»˜è®¤è§’è‰²"""
        roles = [
            "ä¸€åä¸­å­¦ç”Ÿ",
            "å¤§å­¦ç›¸å…³ä¸“ä¸šæ•™æˆ",
            "è¡Œä¸šåˆ†æå¸ˆ",
            "å¯¹æ­¤è¯é¢˜æŒä¹è§‚æ€åº¦çš„æ”¯æŒè€…",
            "æŒè°¨æ…æ€€ç–‘æ€åº¦çš„è§‚å¯Ÿè€…",
            "äº‹ä»¶äº²å†è€…æˆ–å—å½±å“è€…"
        ]
        return roles[:min(num, len(roles))]

    def _get_default_styles(self, num):
        """é»˜è®¤é£æ ¼"""
        styles = [
            "ç®€æ´å®¢è§‚ï¼Œå¹³å®ç›´è¿°",
            "ä¸¥è°¨å­¦æœ¯ï¼Œå¼•ç”¨æ•°æ®ä¸æ–‡çŒ®",
            "çƒ­æƒ…æ´‹æº¢ï¼Œå¯Œæœ‰æ„ŸæŸ“åŠ›",
            "å¹½é»˜é£è¶£ï¼Œå¸¦ç‚¹è°ƒä¾ƒ",
            "å¨“å¨“é“æ¥ï¼Œå……æ»¡æ•…äº‹æ€§",
            "æ‰¹åˆ¤æ€§æ€ç»´ï¼Œå¤šè§’åº¦è´¨ç–‘"
        ]
        return styles[:min(num, len(styles))]

    def _get_default_constraints(self, num):
        """é»˜è®¤çº¦æŸ"""
        constraints = [
            "å…¨æ–‡ä¸å°‘äº400å­—",
            "è‡³å°‘åŒ…å«ä¸‰ä¸ªåˆ†è®ºç‚¹æˆ–ä¸‰ä¸ªæ ¸å¿ƒç»†èŠ‚",
            "å¼•ç”¨ä¸€ä¸ªçœŸå®çš„æ•°æ®ã€ç ”ç©¶æˆ–å†å²äº‹ä»¶",
            "é¿å…ä½¿ç”¨ä»»ä½•ä¸“ä¸šæœ¯è¯­ï¼ŒåŠ›æ±‚é€šä¿—æ˜“æ‡‚",
            "åœ¨ç»“å°¾å¤„æå‡ºä¸€ä¸ªå¼•äººæ·±æ€çš„é—®é¢˜",
            "ä½¿ç”¨ä¸€ä¸ªç”ŸåŠ¨çš„æ¯”å–»æˆ–ç±»æ¯”æ¥è¯´æ˜æ ¸å¿ƒè§‚ç‚¹"
        ]
        return constraints[:min(num, len(constraints))]


# ==================== 3. æ™ºèƒ½ç»„åˆç”Ÿæˆå™¨ ====================

class IntelligentCombinationGenerator:
    """æ™ºèƒ½ç”Ÿæˆåˆç†çš„å…­ç»´ç»„åˆ"""

    def __init__(self, auto_generator):
        self.auto_gen = auto_generator

    def generate_combinations(self, num_combinations=1000, attributes=None):
        """ç”Ÿæˆæ™ºèƒ½ç»„åˆ"""
        if attributes is None:
            attributes = ["æå†™", "è®°å™", "è¯´æ˜", "æŠ’æƒ…", "è®®è®º"]

        # ç”Ÿæˆæ‰€æœ‰ç»´åº¦
        logger.info("å¼€å§‹ç”Ÿæˆç»´åº¦å†…å®¹...")

        # ä½¿ç”¨ç¼“å­˜æœºåˆ¶
        topics = self.auto_gen.generate_with_cache(
            f"topics_{datetime.now().strftime('%Y%m%d')}",
            self.auto_gen.generate_topics,
            num_topics=50
        )

        genres = self.auto_gen.generate_with_cache(
            f"genres_{datetime.now().strftime('%Y%m%d')}",
            self.auto_gen.generate_genres,
            num_genres=20
        )

        roles = self.auto_gen.generate_with_cache(
            f"roles_{datetime.now().strftime('%Y%m%d')}",
            self.auto_gen.generate_roles,
            num_roles=15
        )

        styles = self.auto_gen.generate_with_cache(
            f"styles_{datetime.now().strftime('%Y%m%d')}",
            self.auto_gen.generate_styles,
            num_styles=10
        )

        constraints = self.auto_gen.generate_with_cache(
            f"constraints_{datetime.now().strftime('%Y%m%d')}",
            self.auto_gen.generate_constraints,
            num_constraints=8
        )

        logger.info(f"ç»´åº¦ç”Ÿæˆå®Œæˆ: è¯é¢˜{len(topics)}ä¸ª, æ–‡ä½“{len(genres)}ä¸ª, "
                    f"è§’è‰²{len(roles)}ä¸ª, é£æ ¼{len(styles)}ä¸ª, çº¦æŸ{len(constraints)}ä¸ª")

        combinations = []

        for i in range(num_combinations):
            # æ™ºèƒ½é€‰æ‹©ï¼šç¡®ä¿åˆç†åŒ¹é…
            attribute = random.choice(attributes)
            topic = random.choice(topics)

            # æ ¹æ®è¯é¢˜é€‰æ‹©åˆé€‚è§’è‰²
            role = self._select_appropriate_role(topic, roles)

            # æ ¹æ®è¯é¢˜å’Œè§’è‰²é€‰æ‹©åˆé€‚æ–‡ä½“
            genre = self._select_appropriate_genre(topic, role, genres)

            # æ ¹æ®æ–‡ä½“é€‰æ‹©åˆé€‚é£æ ¼
            style = self._select_appropriate_style(genre, styles)

            # éšæœºé€‰æ‹©çº¦æŸ
            constraint = random.choice(constraints)

            # ç”Ÿæˆæç¤ºè¯
            prompt = self._generate_prompt(attribute, topic, genre, role, style, constraint)

            # è®¡ç®—è´¨é‡è¯„åˆ†
            quality_score = self._estimate_quality(attribute, topic, genre, role, style, constraint)

            combinations.append({
                "plan_id": i,
                "attribute": attribute,
                "topic": topic,
                "genre": genre,
                "role": role,
                "style": style,
                "constraint": constraint,
                "prompt": prompt,
                "quality_score": quality_score,
                "generated": False  # æ ‡è®°æ˜¯å¦å·²ç”Ÿæˆ
            })

            if (i + 1) % 100 == 0:
                logger.info(f"å·²ç”Ÿæˆ {i + 1}/{num_combinations} ä¸ªç»„åˆ")

        # æŒ‰è´¨é‡è¯„åˆ†æ’åº
        combinations.sort(key=lambda x: x["quality_score"], reverse=True)

        return combinations

    def _select_appropriate_role(self, topic, roles):
        """æ ¹æ®è¯é¢˜é€‰æ‹©åˆé€‚çš„è§’è‰²"""
        # ç®€å•å…³é”®è¯åŒ¹é…
        topic_lower = topic.lower()

        # ç§‘æŠ€è¯é¢˜é€‚åˆæŠ€æœ¯ç›¸å…³è§’è‰²
        tech_keywords = ["äººå·¥", "æ™ºèƒ½", "é‡å­", "è®¡ç®—", "æ•°æ®", "ç®—æ³•", "ç§‘æŠ€", "æŠ€æœ¯", "æ•°å­—"]
        if any(kw in topic_lower for kw in tech_keywords):
            tech_roles = [r for r in roles if any(kw in r.lower() for kw in
                                                  ["æŠ€æœ¯", "ç§‘å­¦", "å·¥ç¨‹", "ç ”ç©¶", "å¼€å‘"])]
            if tech_roles:
                return random.choice(tech_roles)

        # ç¤¾ä¼šè¯é¢˜é€‚åˆæ™®é€šå…¬ä¼—è§’è‰²
        social_keywords = ["ç¤¾ä¼š", "æ°‘ç”Ÿ", "ç”Ÿæ´»", "ç¤¾åŒº", "å®¶åº­", "æ¶ˆè´¹", "å…»è€"]
        if any(kw in topic_lower for kw in social_keywords):
            social_roles = [r for r in roles if any(kw in r.lower() for kw in
                                                    ["å¸‚æ°‘", "å±…æ°‘", "æ¶ˆè´¹è€…", "è€äºº", "é’å¹´"])]
            if social_roles:
                return random.choice(social_roles)

        # ç»æµè¯é¢˜é€‚åˆå•†ä¸šç›¸å…³è§’è‰²
        economic_keywords = ["ç»æµ", "å•†ä¸š", "å¸‚åœº", "é‡‘è", "åˆ›ä¸š", "æŠ•èµ„", "è¥é”€"]
        if any(kw in topic_lower for kw in economic_keywords):
            economic_roles = [r for r in roles if any(kw in r.lower() for kw in
                                                      ["åˆ†æ", "ä¼ä¸š", "å•†ä¸š", "æŠ•èµ„", "å¸‚åœº"])]
            if economic_roles:
                return random.choice(economic_roles)

        # é»˜è®¤éšæœºé€‰æ‹©
        return random.choice(roles)

    def _select_appropriate_genre(self, topic, role, genres):
        """æ ¹æ®è¯é¢˜å’Œè§’è‰²é€‰æ‹©åˆé€‚çš„æ–‡ä½“"""
        role_lower = role.lower()

        # å­¦ç”Ÿè§’è‰²é€‚åˆæ•™è‚²ç±»æ–‡ä½“
        if any(kw in role_lower for kw in ["å­¦ç”Ÿ", "å­©å­", "é’å°‘å¹´"]):
            educational_genres = [g for g in genres if any(kw in g.lower() for kw in
                                                           ["ä½œæ–‡", "æ—¥è®°", "è¯»åæ„Ÿ", "å‘¨è®°"])]
            if educational_genres:
                return random.choice(educational_genres)

        # ä¸“ä¸šè§’è‰²é€‚åˆä¸“ä¸šæ–‡ä½“
        if any(kw in role_lower for kw in ["æ•™æˆ", "ä¸“å®¶", "åˆ†æ", "ç ”ç©¶", "å·¥ç¨‹å¸ˆ"]):
            professional_genres = [g for g in genres if any(kw in g.lower() for kw in
                                                            ["è®ºæ–‡", "æŠ¥å‘Š", "åˆ†æ", "ç ”ç©¶", "å­¦æœ¯"])]
            if professional_genres:
                return random.choice(professional_genres)

        # æ™®é€šå…¬ä¼—è§’è‰²é€‚åˆåª’ä½“æ–‡ä½“
        if any(kw in role_lower for kw in ["æ¶ˆè´¹è€…", "å¸‚æ°‘", "ç”¨æˆ·", "è¯»è€…", "äº²å†è€…"]):
            media_genres = [g for g in genres if any(kw in g.lower() for kw in
                                                     ["åšå®¢", "æ–‡ç« ", "è¯„è®º", "é—®ç­”", "æ¨æ–‡"])]
            if media_genres:
                return random.choice(media_genres)

        # é»˜è®¤éšæœºé€‰æ‹©
        return random.choice(genres)

    def _select_appropriate_style(self, genre, styles):
        """æ ¹æ®æ–‡ä½“é€‰æ‹©åˆé€‚çš„é£æ ¼"""
        genre_lower = genre.lower()

        # æ­£å¼æ–‡ä½“é€‚åˆä¸¥è°¨é£æ ¼
        if any(kw in genre_lower for kw in ["å­¦æœ¯", "æŠ¥å‘Š", "è®ºæ–‡", "æ­£å¼", "å•†ä¸š", "åˆ†æ"]):
            formal_styles = [s for s in styles if any(kw in s.lower() for kw in
                                                      ["ä¸¥è°¨", "å®¢è§‚", "å­¦æœ¯", "æ­£å¼", "æ‰¹åˆ¤"])]
            if formal_styles:
                return random.choice(formal_styles)

        # åª’ä½“æ–‡ä½“é€‚åˆæ´»æ³¼é£æ ¼
        if any(kw in genre_lower for kw in ["å¾®ä¿¡", "åšå®¢", "æ—¥è®°", "æ¸¸è®°", "æ•…äº‹", "æ¨æ–‡"]):
            informal_styles = [s for s in styles if any(kw in s.lower() for kw in
                                                        ["å¹½é»˜", "çƒ­æƒ…", "æ•…äº‹", "äº²åˆ‡", "å¨“å¨“é“æ¥"])]
            if informal_styles:
                return random.choice(informal_styles)

        # åˆ›æ„æ–‡ä½“é€‚åˆæ–‡å­¦é£æ ¼
        if any(kw in genre_lower for kw in ["è¯—æ­Œ", "å°è¯´", "æ•£æ–‡", "åˆ›æ„"]):
            creative_styles = [s for s in styles if any(kw in s.lower() for kw in
                                                        ["è¯—æ„", "æ•…äº‹", "æŠ’æƒ…", "æ„ŸæŸ“åŠ›"])]
            if creative_styles:
                return random.choice(creative_styles)

        # é»˜è®¤éšæœºé€‰æ‹©
        return random.choice(styles)

    def _generate_prompt(self, attribute, topic, genre, role, style, constraint):
        """ç”Ÿæˆå…­ç»´æç¤ºè¯"""
        attribute_instructions = {
            "æå†™": f"è¯·å¯¹ä»¥ä¸‹ä¸»é¢˜è¿›è¡Œå…·ä½“ã€ç»†è…»çš„**æå†™**ï¼Œèšç„¦äºæ„Ÿå®˜ç»†èŠ‚å’Œç”»é¢è¥é€ ï¼š",
            "è®°å™": f"è¯·å›´ç»•ä»¥ä¸‹ä¸»é¢˜ï¼Œ**è®°å™**ä¸€ä¸ªå®Œæ•´çš„äº‹ä»¶æˆ–æ•…äº‹ï¼Œè®²æ¸…æ¥é¾™å»è„‰ï¼š",
            "è¯´æ˜": f"è¯·å¯¹ä»¥ä¸‹ä¸»é¢˜è¿›è¡Œå®¢è§‚ã€æ¸…æ™°ã€æœ‰æ¡ç†çš„**è¯´æ˜**ï¼Œè§£é‡Šå…¶äº‹å®æˆ–åŸç†ï¼š",
            "æŠ’æƒ…": f"è¯·å°±ä»¥ä¸‹ä¸»é¢˜ï¼Œ**æŠ’å‘**ä½ çœŸå®çš„æƒ…æ„Ÿã€æ„Ÿå—æˆ–æ€è€ƒï¼š",
            "è®®è®º": f"è¯·å°±ä»¥ä¸‹ä¸»é¢˜ï¼Œå‘è¡¨æ˜ç¡®çš„è§‚ç‚¹å¹¶è¿›è¡Œæœ‰åŠ›çš„**è®®è®º**å’Œè®ºè¯ï¼š"
        }

        parts = []
        parts.append(f"ã€ä»»åŠ¡ã€‘{attribute_instructions.get(attribute, '')}")
        parts.append(f"ã€ä¸»é¢˜ã€‘{topic}")
        parts.append(f"ã€ä½ çš„è§’è‰²ã€‘è¯·ä»¥{role}çš„èº«ä»½è¿›è¡Œå†™ä½œã€‚")
        parts.append(f"ã€æ–‡ä½“ä¸å‘å¸ƒå¹³å°ã€‘è¯·å†™æˆä¸€ç¯‡{genre}ã€‚")
        parts.append(f"ã€è¯­è¨€é£æ ¼ã€‘æ•´ä½“æ–‡é£è¯·ä¿æŒ{style}ã€‚")
        parts.append(f"ã€ç‰¹æ®Šè¦æ±‚ã€‘{constraint}ã€‚")

        final_prompt = "\n".join(parts)
        final_prompt += "\n\nè¯·ç»¼åˆä»¥ä¸Šæ‰€æœ‰è¦æ±‚ï¼Œåˆ›ä½œä¸€ç¯‡å®Œæ•´ã€è¿è´¯çš„æ–‡ç« ã€‚"
        return final_prompt

    def _estimate_quality(self, attribute, topic, genre, role, style, constraint):
        """ä¼°è®¡ç»„åˆçš„è´¨é‡åˆ†æ•°ï¼ˆ0-1ï¼‰"""
        score = 0.5  # åŸºç¡€åˆ†

        # è¯é¢˜è´¨é‡åŠ åˆ†
        if len(topic) > 10 and len(topic) < 80:
            score += 0.1

        # è§’è‰²ä¸è¯é¢˜åŒ¹é…åº¦åŠ åˆ†
        if self._check_compatibility(role, topic):
            score += 0.2

        # æ–‡ä½“ä¸é£æ ¼åŒ¹é…åº¦åŠ åˆ†
        if self._check_genre_style_match(genre, style):
            score += 0.1

        # é¿å…è¿‡äºå¤æ‚çš„ç»„åˆ
        if len(constraint) < 50:
            score += 0.05

        # å¤šæ ·æ€§åŠ åˆ†
        score += random.uniform(0, 0.05)

        return min(score, 1.0)

    def _check_compatibility(self, role, topic):
        """æ£€æŸ¥è§’è‰²ä¸è¯é¢˜çš„å…¼å®¹æ€§"""
        tech_keywords = ["ç§‘æŠ€", "AI", "äººå·¥", "æ™ºèƒ½", "æ•°æ®", "ç®—æ³•", "é‡å­", "æ•°å­—"]
        if any(kw in topic for kw in tech_keywords):
            if any(kw in role for kw in ["æŠ€æœ¯", "ç§‘å­¦", "å·¥ç¨‹", "ç ”ç©¶", "å¼€å‘"]):
                return True

        social_keywords = ["ç¤¾ä¼š", "æ°‘ç”Ÿ", "ç”Ÿæ´»", "ç¤¾åŒº", "æ–‡åŒ–", "å®¶åº­"]
        if any(kw in topic for kw in social_keywords):
            if any(kw in role for kw in ["å¸‚æ°‘", "å±…æ°‘", "è§‚å¯Ÿè€…", "äº²å†è€…", "æ¶ˆè´¹è€…"]):
                return True

        return False

    def _check_genre_style_match(self, genre, style):
        """æ£€æŸ¥æ–‡ä½“ä¸é£æ ¼çš„åŒ¹é…åº¦"""
        formal_keywords = ["å­¦æœ¯", "æŠ¥å‘Š", "è®ºæ–‡", "æ­£å¼", "å•†ä¸š", "åˆ†æ"]
        if any(kw in genre for kw in formal_keywords):
            if any(kw in style for kw in ["ä¸¥è°¨", "å®¢è§‚", "å­¦æœ¯", "æ­£å¼", "æ‰¹åˆ¤"]):
                return True

        informal_keywords = ["å¾®ä¿¡", "åšå®¢", "æ—¥è®°", "æ¸¸è®°", "æ•…äº‹", "æ¨æ–‡"]
        if any(kw in genre for kw in informal_keywords):
            if any(kw in style for kw in ["å¹½é»˜", "çƒ­æƒ…", "æ•…äº‹", "äº²åˆ‡", "å¨“å¨“é“æ¥"]):
                return True

        return False


# ==================== 4. å¢å¼ºå‹APIè°ƒç”¨å™¨ ====================

def generate_text_with_retry(prompt: str, target_model: str, max_retries: int = 3) -> Optional[str]:
    """è°ƒç”¨æŒ‡å®šæ¨¡å‹ç”Ÿæˆæ–‡æœ¬ï¼Œå¤±è´¥æ—¶è‡ªåŠ¨é‡è¯•"""
    if target_model not in clients or clients[target_model] is None:
        print(f"    âš ï¸  æ¨¡å‹ {target_model} æœªé…ç½®ï¼Œè·³è¿‡")
        return None

    config = MODEL_CONFIGS[target_model]
    for attempt in range(max_retries):
        try:
            response = clients[target_model].chat.completions.create(
                model=config["model_name"],
                messages=[{"role": "user", "content": prompt}],
                max_tokens=2000,
                temperature=0.7,
                stream=False
            )
            return response.choices[0].message.content.strip()

        except openai.APIError as e:
            wait_time = (attempt + 1) * 5  # æŒ‡æ•°é€€é¿
            print(
                f"    âš ï¸  {target_model.upper()} APIé”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {e}. {wait_time}ç§’åé‡è¯•...")
            time.sleep(wait_time)
        except Exception as e:
            print(f"    âŒ  {target_model.upper()} éé¢„æœŸé”™è¯¯: {e}")
            return None

    print(f"    âŒ  {target_model.upper()} åœ¨{max_retries}æ¬¡é‡è¯•åä»å¤±è´¥")
    return None


# ==================== 5. æ™ºèƒ½è´¨é‡è¯„ä¼°å™¨ ====================

class TextQualityAssessor:
    """è¯„ä¼°ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡"""

    def __init__(self, client):
        self.client = client

    def assess_quality(self, text, prompt_info):
        """è¯„ä¼°æ–‡æœ¬è´¨é‡"""
        if not text or len(text.strip()) < 50:
            return 0.0

        # åŸºç¡€è´¨é‡æ£€æŸ¥
        score = 0.5

        # é•¿åº¦æ£€æŸ¥
        expected_min_length = 300 if "ä¸å°‘äº400å­—" in prompt_info.get("constraint", "") else 100
        if len(text) >= expected_min_length:
            score += 0.2

        # ç»“æ„æ£€æŸ¥
        if '\n\n' in text or text.count('ã€‚') >= 3:
            score += 0.1

        # å†…å®¹æ£€æŸ¥
        if "ä¸‰ä¸ªåˆ†è®ºç‚¹" in prompt_info.get("constraint", ""):
            if text.count('é¦–å…ˆ') + text.count('å…¶æ¬¡') + text.count('å†æ¬¡') + text.count('ç¬¬ä¸€') + text.count(
                    'ç¬¬äºŒ') + text.count('ç¬¬ä¸‰') >= 2:
                score += 0.1

        if "æ¯”å–»" in prompt_info.get("constraint", "") or "ç±»æ¯”" in prompt_info.get("constraint", ""):
            if "åƒ" in text or "å¦‚åŒ" in text or "å¥½æ¯”" in text:
                score += 0.1

        # é¿å…AIæ¨¡æ¿å›å¤
        if "ä½œä¸ºä¸€ä¸ªAI" not in text and "å¾ˆæŠ±æ­‰" not in text and "æ— æ³•å®Œæˆ" not in text:
            score += 0.1

        return min(score, 1.0)


# ==================== 6. ä¸»æ§åˆ¶å™¨ï¼šå®Œå…¨è‡ªåŠ¨åŒ–ç”Ÿæˆ ====================

def main_auto():
    """å®Œå…¨è‡ªåŠ¨åŒ–çš„æ•°æ®é›†ç”Ÿæˆç³»ç»Ÿ"""
    print("=" * 70)
    print("          å®Œå…¨è‡ªåŠ¨åŒ–å…­ç»´æ–‡æœ¬æ•°æ®é›†ç”Ÿæˆç³»ç»Ÿ")
    print("=" * 70)

    # æ£€æŸ¥å¯ç”¨çš„å®¢æˆ·ç«¯
    active_clients = {k: v for k, v in clients.items() if v is not None}
    if not active_clients:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„APIå®¢æˆ·ç«¯")
        return

    print(f"âœ… å¯ç”¨çš„æ¨¡å‹ï¼š{list(active_clients.keys())}")

    # é…ç½®ç”Ÿæˆå‚æ•°
    SAMPLES_PER_MODEL = 800  # æ¯ä¸ªæ¨¡å‹ç”Ÿæˆ800æ¡æ•°æ®
    ACTIVE_MODELS = list(active_clients.keys())  # ä½¿ç”¨æ‰€æœ‰å¯ç”¨æ¨¡å‹
    OUTPUT_DIR = "auto_generated_datasets"
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # åˆå§‹åŒ–ç»„ä»¶ï¼ˆä¼˜å…ˆä½¿ç”¨customå®¢æˆ·ç«¯ï¼‰
    primary_client = active_clients.get("custom") or active_clients.get("deepseek") or list(active_clients.values())[0]
    auto_gen = AutoDimensionGenerator(primary_client)
    comb_gen = IntelligentCombinationGenerator(auto_gen)
    quality_assessor = TextQualityAssessor(primary_client)

    # ç”Ÿæˆç»„åˆè®¡åˆ’
    plan_file = os.path.join(OUTPUT_DIR, "auto_generation_plan.json")

    if os.path.exists(plan_file):
        logger.info("æ£€æµ‹åˆ°å·²æœ‰ç”Ÿæˆè®¡åˆ’ï¼Œå°†ä»ä¸­æ–­å¤„ç»§ç»­...")
        with open(plan_file, 'r', encoding='utf-8') as f:
            generation_plan = json.load(f)
    else:
        logger.info("åˆ›å»ºæ–°çš„ç”Ÿæˆè®¡åˆ’...")
        generation_plan = comb_gen.generate_combinations(
            num_combinations=SAMPLES_PER_MODEL,
            attributes=["æå†™", "è®°å™", "è¯´æ˜", "æŠ’æƒ…", "è®®è®º"]
        )

        # ä¿å­˜è®¡åˆ’
        with open(plan_file, 'w', encoding='utf-8') as f:
            json.dump(generation_plan, f, ensure_ascii=False, indent=2)

        logger.info(f"ç”Ÿæˆè®¡åˆ’å·²ä¿å­˜ï¼Œå…±{len(generation_plan)}æ¡ä»»åŠ¡ã€‚")
        print(f"ğŸ“Š ç”Ÿæˆè®¡åˆ’è´¨é‡åˆ†å¸ƒï¼š")
        scores = [item["quality_score"] for item in generation_plan]
        print(f"   å¹³å‡è´¨é‡: {sum(scores) / len(scores):.2f}")
        print(f"   æœ€é«˜è´¨é‡: {max(scores):.2f}")
        print(f"   æœ€ä½è´¨é‡: {min(scores):.2f}")

    # ä¸ºæ¯ä¸ªæ¨¡å‹ç”Ÿæˆæ•°æ®
    for model_name in ACTIVE_MODELS:
        if model_name not in active_clients:
            print(f"\nâ­ï¸  è·³è¿‡æ¨¡å‹ {model_name.upper()} (é…ç½®æ— æ•ˆ)")
            continue

        print(f"\n{'=' * 50}")
        print(f"å¼€å§‹ä¸ºæ¨¡å‹ {model_name.upper()} ç”Ÿæˆæ•°æ®")
        print(f"{'=' * 50}")

        output_file = os.path.join(OUTPUT_DIR,
                                   f"auto_dataset_{model_name}_{datetime.now().strftime('%Y%m%d_%H%M')}.csv")
        data_records = []
        failed_count = 0
        success_count = 0
        total_quality = 0

        # åªå¤„ç†æœªç”Ÿæˆçš„é«˜è´¨é‡ç»„åˆ
        ungenerated_plans = [p for p in generation_plan if not p.get(f"generated_{model_name}", False)]

        # æŒ‰è´¨é‡æ’åºï¼Œå…ˆç”Ÿæˆé«˜è´¨é‡çš„ç»„åˆ
        ungenerated_plans.sort(key=lambda x: x.get("quality_score", 0), reverse=True)

        for idx, plan_item in enumerate(ungenerated_plans[:SAMPLES_PER_MODEL]):
            print(f"[{model_name.upper()} {idx + 1}/{min(len(ungenerated_plans), SAMPLES_PER_MODEL)}] "
                  f"è´¨é‡:{plan_item['quality_score']:.2f} å±æ€§:{plan_item['attribute']} ä¸»é¢˜:{plan_item['topic'][:15]}...")

            text = generate_text_with_retry(plan_item["prompt"], model_name)
            time.sleep(0.5)  # æ§åˆ¶è¯·æ±‚é¢‘ç‡

            if text:
                # è¯„ä¼°è´¨é‡
                quality = quality_assessor.assess_quality(text, plan_item)
                total_quality += quality
                success_count += 1

                record = {
                    "text_id": f"{model_name}_{plan_item['plan_id']}",
                    "text_content": text,
                    "source_model": model_name,
                    "attribute": plan_item['attribute'],
                    "topic": plan_item['topic'],
                    "genre": plan_item['genre'],
                    "role": plan_item['role'],
                    "style": plan_item['style'],
                    "constraint": plan_item['constraint'],
                    "prompt": plan_item['prompt'],
                    "combination_quality": plan_item['quality_score'],
                    "generation_quality": quality,
                    "length": len(text),
                    "timestamp": datetime.now().isoformat()
                }
                data_records.append(record)
                plan_item[f"generated_{model_name}"] = True  # æ ‡è®°ä¸ºå·²ç”Ÿæˆ

                quality_str = f"è´¨é‡:{quality:.2f}"
                if quality > 0.8:
                    quality_str = f"âœ… ä¼˜ç§€ {quality_str}"
                elif quality > 0.6:
                    quality_str = f"âœ“ è‰¯å¥½ {quality_str}"
                else:
                    quality_str = f"âš ï¸ ä¸€èˆ¬ {quality_str}"

                print(f"    {quality_str} ({len(text)} å­—ç¬¦)")
            else:
                failed_count += 1
                print(f"    âŒ å¤±è´¥")

            # æ¯ç”Ÿæˆ20æ¡æˆ–ç»“æŸæ—¶ï¼Œä¿å­˜ä¸€æ¬¡è¿›åº¦
            if (idx + 1) % 20 == 0 or (idx + 1) == min(len(ungenerated_plans), SAMPLES_PER_MODEL):
                # ä¿å­˜æ•°æ®
                if data_records:
                    df = pd.DataFrame(data_records)
                    # å¦‚æœæ˜¯è¿½åŠ æ¨¡å¼ï¼ˆéé¦–æ¬¡ä¿å­˜ï¼‰
                    if os.path.exists(output_file) and idx >= 20:
                        df.to_csv(output_file, mode='a', header=False, index=False, encoding='utf-8-sig')
                    else:
                        df.to_csv(output_file, index=False, encoding='utf-8-sig')
                    data_records = []  # æ¸…ç©ºå†…å­˜

                # æ›´æ–°ç”Ÿæˆè®¡åˆ’ï¼ˆä¿å­˜è¿›åº¦ï¼‰
                with open(plan_file, 'w', encoding='utf-8') as f:
                    json.dump(generation_plan, f, ensure_ascii=False, indent=2)
                print(f"    ğŸ’¾ è¿›åº¦å·²ä¿å­˜")

        avg_quality = total_quality / success_count if success_count > 0 else 0
        print(f"\nğŸ“Š {model_name.upper()} ç”Ÿæˆå®Œæˆï¼")
        print(f"   æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count}")
        print(f"   å¹³å‡ç”Ÿæˆè´¨é‡: {avg_quality:.2f}")
        print(f"ğŸ“ æ•°æ®æ–‡ä»¶: {output_file}")

    # æœ€ç»ˆåˆå¹¶æ‰€æœ‰æ¨¡å‹çš„æ•°æ®
    print(f"\n{'=' * 70}")
    print("æ­£åœ¨åˆå¹¶æ‰€æœ‰æ¨¡å‹çš„æ•°æ®é›†...")
    all_data_files = [f for f in os.listdir(OUTPUT_DIR) if f.startswith("auto_dataset_") and f.endswith(".csv")]
    combined_df = pd.DataFrame()

    for file in all_data_files:
        try:
            df = pd.read_csv(os.path.join(OUTPUT_DIR, file), encoding='utf-8-sig')
            combined_df = pd.concat([combined_df, df], ignore_index=True)
        except Exception as e:
            logger.error(f"è¯»å–æ–‡ä»¶ {file} å¤±è´¥: {e}")

    if not combined_df.empty:
        combined_file = os.path.join(OUTPUT_DIR,
                                     f"AUTO_COMBINED_DATASET_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv")
        combined_df.to_csv(combined_file, index=False, encoding='utf-8-sig')

        print(f"âœ… åˆå¹¶å®Œæˆï¼æ€»è®¡ {len(combined_df)} æ¡æ•°æ®ã€‚")
        print(f"ğŸ“ åˆå¹¶æ–‡ä»¶: {combined_file}")

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“ˆ æ•°æ®è´¨é‡ç»Ÿè®¡:")
        print(f"   å¹³å‡ç»„åˆè´¨é‡: {combined_df['combination_quality'].mean():.2f}")
        print(f"   å¹³å‡ç”Ÿæˆè´¨é‡: {combined_df['generation_quality'].mean():.2f}")

        print(f"\nğŸ“Š æ•°æ®åˆ†å¸ƒç»Ÿè®¡:")
        print("1. æŒ‰æ¨¡å‹æ¥æºåˆ†å¸ƒ:")
        print(combined_df['source_model'].value_counts())

        print("\n2. æŒ‰æ–‡æœ¬å±æ€§åˆ†å¸ƒ:")
        print(combined_df['attribute'].value_counts())

        print("\n3. è¯é¢˜å¤šæ ·æ€§:")
        print(f"   å”¯ä¸€è¯é¢˜æ•°: {combined_df['topic'].nunique()}")

        print("\n4. æ–‡æœ¬é•¿åº¦ç»Ÿè®¡:")
        print(f"   å¹³å‡é•¿åº¦: {combined_df['length'].mean():.0f} å­—ç¬¦")
        print(f"   æœ€çŸ­: {combined_df['length'].min()} å­—ç¬¦")
        print(f"   æœ€é•¿: {combined_df['length'].max()} å­—ç¬¦")

        # ä¿å­˜å…ƒæ•°æ®
        meta_file = os.path.join(OUTPUT_DIR, f"meta_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        meta_data = {
            "generation_time": datetime.now().isoformat(),
            "total_samples": len(combined_df),
            "models_used": list(combined_df['source_model'].unique()),
            "avg_combination_quality": float(combined_df['combination_quality'].mean()),
            "avg_generation_quality": float(combined_df['generation_quality'].mean()),
            "dimension_stats": {
                "attributes": combined_df['attribute'].value_counts().to_dict(),
                "topics_count": int(combined_df['topic'].nunique()),
                "genres": combined_df['genre'].value_counts().to_dict(),
                "roles": combined_df['role'].value_counts().to_dict()
            },
            "text_length_stats": {
                "avg": float(combined_df['length'].mean()),
                "min": int(combined_df['length'].min()),
                "max": int(combined_df['length'].max())
            }
        }

        with open(meta_file, 'w', encoding='utf-8') as f:
            json.dump(meta_data, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ“‹ å…ƒæ•°æ®ä¿å­˜è‡³: {meta_file}")

        # è´¨é‡åˆ†å¸ƒå¯è§†åŒ–
        quality_bins = pd.cut(combined_df['generation_quality'],
                              bins=[0, 0.3, 0.5, 0.7, 0.9, 1.0],
                              labels=['å¾ˆå·®', 'è¾ƒå·®', 'ä¸€èˆ¬', 'è‰¯å¥½', 'ä¼˜ç§€'])
        print(f"\nğŸ“Š ç”Ÿæˆè´¨é‡åˆ†å¸ƒ:")
        print(quality_bins.value_counts().sort_index())

    else:
        print("âŒ æœªæ‰¾åˆ°å¯åˆå¹¶çš„æ•°æ®æ–‡ä»¶ã€‚")


# ==================== 7. ç¨‹åºå…¥å£ ====================

if __name__ == "__main__":
    print("=" * 70)
    print("          æ–‡æœ¬æ•°æ®é›†ç”Ÿæˆç³»ç»Ÿ")
    print("=" * 70)
    print("è¯·é€‰æ‹©ç”Ÿæˆæ¨¡å¼ï¼š")
    print("1. åŸå§‹ç‰ˆæœ¬ï¼ˆé¢„è®¾ç»´åº¦ï¼‰")
    print("2. è‡ªåŠ¨ç‰ˆæœ¬ï¼ˆAIç”Ÿæˆç»´åº¦ï¼Œæ¨èï¼‰")

    choice = input("è¯·è¾“å…¥é€‰æ‹©ï¼ˆ1æˆ–2ï¼‰ï¼š").strip()

    if choice == "1":
        # æ³¨æ„ï¼šåŸå§‹ç‰ˆæœ¬éœ€è¦å¯¼å…¥åŸå§‹ä»£ç ä¸­çš„mainå‡½æ•°
        # ç”±äºåŸå§‹ä»£ç å·²ç»è¢«ä¿®æ”¹ï¼Œè¿™é‡Œæä¾›ä¸€ä¸ªå…¼å®¹æ–¹å¼
        print("\nâš ï¸  æ³¨æ„ï¼šåŸå§‹ç‰ˆæœ¬éœ€è¦å•ç‹¬è¿è¡ŒåŸä»£ç æ–‡ä»¶")
        print("æ­£åœ¨å¯åŠ¨è‡ªåŠ¨åŒ–ç‰ˆæœ¬...")
        main_auto()
    else:
        main_auto()