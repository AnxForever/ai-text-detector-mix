# AIæ–‡æœ¬æ£€æµ‹æ¨¡å‹å¹¿æ³›æ€§æ”¹è¿›æ–¹æ¡ˆ
## åŸºäº2025å¹´æœ€æ–°ç ”ç©¶çš„å®Œå–„è®¡åˆ’

---

## ğŸ“Š å½“å‰é¡¹ç›®çŠ¶æ€

**ä¼˜åŠ¿ï¼š**
- âœ… æµ‹è¯•å‡†ç¡®ç‡ï¼š100%
- âœ… å„é•¿åº¦åŒºé—´æ€§èƒ½ç¨³å®š
- âœ… å·²ä¸‹è½½çœŸå®äººç±»æ•°æ®ï¼ˆTHUCNews 9,000æ¡ï¼‰

**æ½œåœ¨å±€é™ï¼š**
- âš ï¸ å¯èƒ½å­˜åœ¨æ ¼å¼åå·®
- âš ï¸ æ³›åŒ–èƒ½åŠ›æœªå……åˆ†æµ‹è¯•
- âš ï¸ å¯¹æŠ—é²æ£’æ€§æœªçŸ¥
- âš ï¸ ä»…è®­ç»ƒæ£€æµ‹2-3ä¸ªæ¨¡å‹çš„è¾“å‡º

---

## ğŸ¯ æå‡å¹¿æ³›æ€§çš„5å¤§ç»´åº¦

åŸºäº2025å¹´æœ€æ–°ç ”ç©¶ï¼Œæˆ‘ä»¬éœ€è¦åœ¨ä»¥ä¸‹ç»´åº¦æå‡æ¨¡å‹ï¼š

### 1ï¸âƒ£ **è·¨åŸŸæ³›åŒ–ï¼ˆCross-Domain Generalizationï¼‰**
- åœ¨ä¸åŒä¸»é¢˜ã€ä¸åŒé¢†åŸŸçš„æ–‡æœ¬ä¸Šè¡¨ç°ä¸€è‡´

### 2ï¸âƒ£ **è·¨æ¨¡å‹æ³›åŒ–ï¼ˆModel-Agnostic Detectionï¼‰**
- æ£€æµ‹æœªè§è¿‡çš„ç”Ÿæˆæ¨¡å‹çš„è¾“å‡º

### 3ï¸âƒ£ **å¯¹æŠ—é²æ£’æ€§ï¼ˆAdversarial Robustnessï¼‰**
- æŠµæŠ—æ”¹å†™ã€åŒä¹‰è¯æ›¿æ¢ç­‰æ”»å‡»

### 4ï¸âƒ£ **é›¶æ ·æœ¬èƒ½åŠ›ï¼ˆZero-Shot Capabilityï¼‰**
- å¯¹æ–°å‡ºç°çš„AIæ¨¡å‹ä»æœ‰æ£€æµ‹èƒ½åŠ›

### 5ï¸âƒ£ **å¤šè¯­è¨€æ³›åŒ–ï¼ˆMultilingual Generalizationï¼‰**
- æ‰©å±•åˆ°å…¶ä»–è¯­è¨€ï¼ˆå¯é€‰ï¼‰

---

## ğŸ“‹ å®Œå–„è®¡åˆ’ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰

### ğŸ”¥ Phase 1: æ•°æ®å±‚é¢æ”¹è¿›ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰

#### Task 1.1ï¼šä½¿ç”¨çœŸå®äººç±»æ•°æ®é‡æ–°è®­ç»ƒ â­â­â­â­â­
**å½“å‰é—®é¢˜ï¼š** å¯èƒ½ä½¿ç”¨äº†AIç”Ÿæˆçš„"æ¨¡æ¿äººç±»æ•°æ®"

**è¡ŒåŠ¨è®¡åˆ’ï¼š**
```bash
# 1. æ›¿æ¢æ•°æ®æº
cd /mnt/c/datacollection

# 2. é‡æ–°ç”ŸæˆBERTæ•°æ®é›†
python scripts/bert_prep/label_and_merge.py \
  --ai-data datasets/final/parallel_dataset_cleaned.csv \
  --human-data datasets/human_texts/thucnews_real_human_9000.csv \
  --output datasets/raw/parallel_dataset_real_human.csv

# 3. é‡æ–°åˆ†å±‚é‡‡æ ·
python scripts/bert_prep/split_dataset.py \
  --input datasets/raw/parallel_dataset_real_human.csv \
  --output-dir datasets/bert_real_human

# 4. é‡æ–°è®­ç»ƒ
python scripts/training/train_bert_improved.py \
  --data-dir datasets/bert_real_human \
  --output-dir models/bert_real_human
```

**é¢„æœŸæ•ˆæœï¼š**
- æ¶ˆé™¤"æ¨¡æ¿ç—•è¿¹"
- æå‡å¯¹çœŸå®äººç±»æ–‡æœ¬çš„è¯†åˆ«èƒ½åŠ›
- å‡†ç¡®ç‡å¯èƒ½é™è‡³85-95%ï¼ˆè¿™æ˜¯å¥½äº‹ï¼Œè¯´æ˜ä¸å†ä¾èµ–è¡¨é¢ç‰¹å¾ï¼‰

**æ—¶é—´æˆæœ¬ï¼š** 3-4å°æ—¶

---

#### Task 1.2ï¼šæ‰©å……å¤šæ¨¡å‹AIæ•°æ® â­â­â­â­
**å½“å‰é—®é¢˜ï¼š** ä»…ä½¿ç”¨DeepSeekå’Œé€šä¹‰åƒé—®ç”Ÿæˆçš„æ•°æ®

**è¡ŒåŠ¨è®¡åˆ’ - æ”¶é›†æ›´å¤šæ¨¡å‹çš„è¾“å‡ºï¼š**

**æ–¹æ¡ˆAï¼šä½¿ç”¨å…è´¹APIï¼ˆæ¨èï¼‰**
```python
# æ·»åŠ æ›´å¤šå…è´¹æ¨¡å‹
models_to_add = [
    "gpt-3.5-turbo",       # OpenAIï¼ˆæœ‰å…è´¹é¢åº¦ï¼‰
    "claude-3-haiku",      # Anthropicï¼ˆé™å…ï¼‰
    "gemini-pro",          # Google
    "llama-3.1-8b",        # Metaï¼ˆé€šè¿‡replicateï¼‰
    "yi-34b-chat",         # 01.AI
    "baichuan2-13b",       # ç™¾å·
    "chatglm3-6b"          # æ™ºè°±AI
]

# ä¿®æ”¹ scripts/data_generation/multi_model_generator.py
# ä¸ºæ¯ä¸ªæ¨¡å‹ç”Ÿæˆ1000-2000æ¡æ ·æœ¬
```

**æ–¹æ¡ˆBï¼šä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼ˆå¦‚æœæœ‰GPUï¼‰**
```bash
# ä½¿ç”¨Ollamaè¿è¡Œæœ¬åœ°æ¨¡å‹
ollama pull llama3.1
ollama pull mistral
ollama pull qwen2.5

# ç¼–å†™è„šæœ¬è°ƒç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆ
```

**æ•°æ®æ”¶é›†ç­–ç•¥ï¼š**
- æ¯ä¸ªæ¨¡å‹ç”Ÿæˆ1,000-1,500æ¡
- ä¿æŒä¸»é¢˜åˆ†å¸ƒä¸€è‡´
- é•¿åº¦èŒƒå›´300-3000å­—ç¬¦

**é¢„æœŸæ•ˆæœï¼š**
- è®­ç»ƒæ•°æ®åŒ…å«8-10ä¸ªä¸åŒæ¨¡å‹çš„è¾“å‡º
- æ¨¡å‹å­¦ä¹ "AIç—•è¿¹"è€Œéç‰¹å®šæ¨¡å‹ç‰¹å¾
- è·¨æ¨¡å‹æ³›åŒ–èƒ½åŠ›å¤§å¹…æå‡

**æ—¶é—´æˆæœ¬ï¼š** 5-8å°æ—¶ï¼ˆå–å†³äºAPIé€Ÿåº¦ï¼‰

---

#### Task 1.3ï¼šæ•°æ®å¢å¼º â­â­â­
**åŸºäº2025å¹´ç ”ç©¶ï¼šBack-translation + Paraphrasing**

**å®æ–½æ–¹æ¡ˆï¼š**
```python
# scripts/data_augmentation/augment_with_llm.py

def augment_dataset(original_df, target_multiplier=1.5):
    """
    ä½¿ç”¨LLMæ”¹å†™æ‰©å……æ•°æ®é›†

    ç­–ç•¥ï¼š
    1. Back-translationï¼ˆä¸­æ–‡â†’è‹±æ–‡â†’ä¸­æ–‡ï¼‰
    2. Paraphrasingï¼ˆä½¿ç”¨Qwen/DeepSeekæ”¹å†™ï¼‰
    3. ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§æ£€æŸ¥
    """

    augmented = []
    for idx, row in original_df.iterrows():
        original_text = row['text']

        # æ”¹å†™1ï¼šBack-translation
        paraphrase1 = back_translate(original_text)

        # æ”¹å†™2ï¼šLLMæ”¹å†™
        paraphrase2 = llm_paraphrase(original_text)

        # è¯­ä¹‰ç›¸ä¼¼åº¦æ£€æŸ¥ï¼ˆ>0.85ä¿ç•™ï¼‰
        if semantic_similarity(original_text, paraphrase1) > 0.85:
            augmented.append({...})

        if semantic_similarity(original_text, paraphrase2) > 0.85:
            augmented.append({...})

    return augmented

# æ‰§è¡Œå¢å¼º
python scripts/data_augmentation/augment_with_llm.py \
  --input datasets/bert_real_human/train.csv \
  --output datasets/bert_augmented/train.csv \
  --multiplier 1.5
```

**é¢„æœŸæ•ˆæœï¼š**
- è®­ç»ƒæ•°æ®ä»14,700æ¡æ‰©å……è‡³22,000æ¡
- æå‡æ¨¡å‹å¯¹æ”¹å†™æ–‡æœ¬çš„é²æ£’æ€§
- é™ä½è¿‡æ‹Ÿåˆé£é™©

**æ—¶é—´æˆæœ¬ï¼š** 4-6å°æ—¶

---

### ğŸ›¡ï¸ Phase 2: æ¨¡å‹å±‚é¢æ”¹è¿›ï¼ˆä¸­ä¼˜å…ˆçº§ï¼‰

#### Task 2.1ï¼šå¯¹æŠ—è®­ç»ƒ â­â­â­â­
**åŸºäºç ”ç©¶ï¼šPIFEæ¡†æ¶ï¼ˆ2025ï¼‰**

**å®æ–½æ–¹æ¡ˆï¼š**
```python
# scripts/training/adversarial_training.py

class AdversarialTrainer:
    """å¯¹æŠ—è®­ç»ƒæ¡†æ¶"""

    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.perturbations = [
            SynonymReplacement(),     # åŒä¹‰è¯æ›¿æ¢
            RandomInsertion(),        # éšæœºæ’å…¥
            RandomSwap(),             # éšæœºäº¤æ¢
            RandomDeletion()          # éšæœºåˆ é™¤
        ]

    def train_step(self, batch):
        """å¯¹æŠ—è®­ç»ƒæ­¥éª¤"""
        # 1. æ ‡å‡†è®­ç»ƒ
        loss_clean = self.forward(batch)

        # 2. ç”Ÿæˆå¯¹æŠ—æ ·æœ¬
        adv_batch = self.generate_adversarial(batch)

        # 3. å¯¹æŠ—è®­ç»ƒ
        loss_adv = self.forward(adv_batch)

        # 4. ç»„åˆæŸå¤±
        total_loss = loss_clean + 0.5 * loss_adv

        return total_loss

    def generate_adversarial(self, batch):
        """ç”Ÿæˆå¯¹æŠ—æ ·æœ¬"""
        perturbed_texts = []
        for text in batch['texts']:
            # éšæœºé€‰æ‹©æ‰°åŠ¨æ–¹æ³•
            perturb = random.choice(self.perturbations)
            perturbed = perturb.apply(text)
            perturbed_texts.append(perturbed)
        return perturbed_texts

# æ‰§è¡Œå¯¹æŠ—è®­ç»ƒ
python scripts/training/adversarial_training.py \
  --data-dir datasets/bert_augmented \
  --output-dir models/bert_adversarial \
  --epochs 5
```

**é¢„æœŸæ•ˆæœï¼š**
- å¯¹åŒä¹‰è¯æ›¿æ¢çš„é²æ£’æ€§æå‡30-50%
- å¯¹æ”¹å†™æ”»å‡»çš„é˜²å¾¡èƒ½åŠ›æ˜¾è‘—å¢å¼º
- å‚è€ƒPIFEæ¡†æ¶ï¼šçœŸé˜³æ€§ç‡ä»48.8%æå‡è‡³82.6%

**æ—¶é—´æˆæœ¬ï¼š** 3-4å°æ—¶

---

#### Task 2.2ï¼šé›†æˆå­¦ä¹  â­â­â­
**åŸºäºç ”ç©¶ï¼š2025å¹´é›†æˆæ–¹æ³•æ€§èƒ½æå‡12%**

**å®æ–½æ–¹æ¡ˆï¼š**
```python
# scripts/training/ensemble_training.py

class EnsembleDetector:
    """é›†æˆæ£€æµ‹å™¨"""

    def __init__(self):
        self.models = [
            BertDetector(),           # BERT-base
            RobertaDetector(),        # RoBERTa
            MacBertDetector(),        # MacBERT
            StatisticalDetector()     # ç»Ÿè®¡ç‰¹å¾æ£€æµ‹å™¨
        ]

    def predict(self, text):
        """é›†æˆé¢„æµ‹"""
        predictions = []
        confidences = []

        for model in self.models:
            pred, conf = model.predict(text)
            predictions.append(pred)
            confidences.append(conf)

        # åŠ æƒæŠ•ç¥¨
        weights = [0.4, 0.3, 0.2, 0.1]  # æ ¹æ®éªŒè¯é›†æ€§èƒ½è°ƒæ•´
        final_pred = np.average(predictions, weights=weights)
        final_conf = np.average(confidences, weights=weights)

        return final_pred, final_conf

# è®­ç»ƒå¤šä¸ªæ¨¡å‹
for model_name in ['bert', 'roberta', 'macbert']:
    python scripts/training/train_multi_model.py \
      --model-type {model_name} \
      --output-dir models/{model_name}_detector
```

**é¢„æœŸæ•ˆæœï¼š**
- å‡†ç¡®ç‡æå‡2-5%
- é²æ£’æ€§æ˜¾è‘—å¢å¼º
- é™ä½å•ä¸€æ¨¡å‹çš„åå·®

**æ—¶é—´æˆæœ¬ï¼š** 6-8å°æ—¶ï¼ˆè®­ç»ƒå¤šä¸ªæ¨¡å‹ï¼‰

---

### ğŸ§ª Phase 3: è¯„ä¼°å±‚é¢æ”¹è¿›ï¼ˆä¸­ä¼˜å…ˆçº§ï¼‰

#### Task 3.1ï¼šæ ¼å¼å¯¹æŠ—æµ‹è¯• â­â­â­â­â­
**åŸºäºä¹‹å‰è®¡åˆ’ï¼š4ç§å¯¹æŠ—åœºæ™¯**

**å®æ–½æ–¹æ¡ˆï¼š**
```bash
# å·²æœ‰è„šæœ¬ï¼Œç›´æ¥è¿è¡Œ
python scripts/evaluation/format_adversarial_test.py \
  --model-dir models/bert_real_human/best_model \
  --test-file datasets/bert_real_human/test.csv \
  --output adversarial_results.json
```

**æµ‹è¯•åœºæ™¯ï¼š**
1. çº¯æ–‡æœ¬æµ‹è¯•ï¼ˆå»é™¤æ‰€æœ‰markdownï¼‰
2. æ ¼å¼åŒ–æµ‹è¯•ï¼ˆæ·»åŠ markdownï¼‰
3. æ ¼å¼äº¤æ¢æµ‹è¯•ï¼ˆAIå»æ ¼å¼ï¼Œäººç±»åŠ æ ¼å¼ï¼‰
4. éšæœºæ ¼å¼æµ‹è¯•

**æˆåŠŸæ ‡å‡†ï¼š**
- å„åœºæ™¯å‡†ç¡®ç‡ä¸‹é™<5%ï¼ˆä¼˜ç§€ï¼‰
- æ ¼å¼äº¤æ¢ä¸‹é™<10%ï¼ˆåˆæ ¼ï¼‰

**æ—¶é—´æˆæœ¬ï¼š** 30åˆ†é’Ÿ

---

#### Task 3.2ï¼šè·¨åŸŸè¯„ä¼° â­â­â­â­
**åŸºäºç ”ç©¶ï¼šSci-SpanDetè·¨å­¦ç§‘æ•°æ®é›†**

**å®æ–½æ–¹æ¡ˆï¼š**
```python
# æ”¶é›†ä¸åŒé¢†åŸŸçš„æµ‹è¯•æ•°æ®
domains = {
    'ç§‘æŠ€': collect_tech_texts(),
    'æ–‡å­¦': collect_literature_texts(),
    'æ–°é—»': collect_news_texts(),
    'å­¦æœ¯': collect_academic_texts(),
    'å¯¹è¯': collect_dialogue_texts()
}

# è·¨åŸŸè¯„ä¼°
for domain, texts in domains.items():
    accuracy = evaluate(model, texts)
    print(f"{domain}: {accuracy:.2%}")

# è®¡ç®—æ–¹å·®ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
variance = np.var(list(accuracies.values()))
```

**æ•°æ®æ”¶é›†ï¼š**
- æ¯ä¸ªé¢†åŸŸ500-1000æ¡æµ‹è¯•æ ·æœ¬
- ä½¿ç”¨ä¸åŒAPIç”Ÿæˆï¼ˆæœªåœ¨è®­ç»ƒä¸­è§è¿‡ï¼‰
- äººç±»æ–‡æœ¬ä»å„é¢†åŸŸè¯­æ–™åº“é‡‡æ ·

**æˆåŠŸæ ‡å‡†ï¼š**
- å„é¢†åŸŸå‡†ç¡®ç‡>80%
- é¢†åŸŸé—´æ–¹å·®<0.05

**æ—¶é—´æˆæœ¬ï¼š** 3-4å°æ—¶

---

#### Task 3.3ï¼šå¯¹æŠ—æ”»å‡»æµ‹è¯• â­â­â­â­
**åŸºäºç ”ç©¶ï¼šAdversarial Paraphrasingï¼ˆ2025ï¼‰**

**å®æ–½æ–¹æ¡ˆï¼š**
```python
# scripts/evaluation/adversarial_attack_test.py

class AdversarialAttackTester:
    """å¯¹æŠ—æ”»å‡»æµ‹è¯•å™¨"""

    def __init__(self, model):
        self.model = model
        self.attacks = {
            'synonym_replacement': self.synonym_attack,
            'back_translation': self.back_trans_attack,
            'paraphrasing': self.paraphrase_attack,
            'word_insertion': self.insertion_attack,
            'word_deletion': self.deletion_attack
        }

    def synonym_attack(self, text, rate=0.1):
        """åŒä¹‰è¯æ›¿æ¢æ”»å‡»"""
        words = text.split()
        n_replace = int(len(words) * rate)
        # æ›¿æ¢n_replaceä¸ªè¯ä¸ºåŒä¹‰è¯
        return attacked_text

    def test_robustness(self, test_df):
        """æµ‹è¯•é²æ£’æ€§"""
        results = {}
        for attack_name, attack_func in self.attacks.items():
            attacked_texts = [attack_func(text) for text in test_df['text']]
            accuracy = self.model.evaluate(attacked_texts, test_df['label'])
            drop = baseline_accuracy - accuracy
            results[attack_name] = {
                'accuracy': accuracy,
                'drop': drop,
                'rating': 'Good' if drop < 0.05 else 'Fair' if drop < 0.10 else 'Poor'
            }
        return results

# æ‰§è¡Œæµ‹è¯•
python scripts/evaluation/adversarial_attack_test.py \
  --model-dir models/bert_adversarial/best_model \
  --test-file datasets/bert_real_human/test.csv
```

**æ”»å‡»ç±»å‹ï¼š**
1. åŒä¹‰è¯æ›¿æ¢ï¼ˆ10%/20%/30%ï¼‰
2. Back-translation
3. LLMæ”¹å†™
4. è¯åºæ‰“ä¹±
5. éšæœºæ’å…¥/åˆ é™¤

**æˆåŠŸæ ‡å‡†ï¼š**
- 10%åŒä¹‰è¯æ›¿æ¢ï¼šå‡†ç¡®ç‡ä¸‹é™<5%
- Back-translationï¼šä¸‹é™<10%
- LLMæ”¹å†™ï¼šä¸‹é™<15%

**æ—¶é—´æˆæœ¬ï¼š** 2-3å°æ—¶

---

### ğŸš€ Phase 4: é«˜çº§ç‰¹æ€§ï¼ˆä½ä¼˜å…ˆçº§ï¼‰

#### Task 4.1ï¼šé›¶æ ·æœ¬æ£€æµ‹èƒ½åŠ› â­â­â­
**åŸºäºç ”ç©¶ï¼šGECScoreæ–¹æ³•ï¼ˆ98.62% AUROCï¼‰**

**æ¦‚å¿µéªŒè¯ï¼š**
```python
# å®ç°åŸºäºè¯­æ³•é”™è¯¯çš„é›¶æ ·æœ¬æ£€æµ‹
class GECScoreDetector:
    """åŸºäºè¯­æ³•çº é”™çš„é›¶æ ·æœ¬æ£€æµ‹å™¨"""

    def __init__(self):
        # åŠ è½½è¯­æ³•çº é”™æ¨¡å‹
        self.gec_model = load_chinese_gec_model()

    def compute_gec_score(self, text):
        """è®¡ç®—GECåˆ†æ•°"""
        # äººç±»æ–‡æœ¬é€šå¸¸æœ‰æ›´å¤šè¯­æ³•é”™è¯¯
        corrected_text = self.gec_model.correct(text)
        edit_distance = levenshtein_distance(text, corrected_text)

        # å½’ä¸€åŒ–
        score = edit_distance / len(text)
        return score

    def predict(self, text):
        """é›¶æ ·æœ¬é¢„æµ‹"""
        score = self.compute_gec_score(text)
        # AIæ–‡æœ¬GECåˆ†æ•°æ›´ä½ï¼ˆæ›´å°‘éœ€è¦çº æ­£ï¼‰
        return 'AI' if score < threshold else 'Human'
```

**å®éªŒè®¾ç½®ï¼š**
- åœ¨å®Œå…¨æœªè§è¿‡çš„æ¨¡å‹è¾“å‡ºä¸Šæµ‹è¯•
- ä¸éœ€è¦é‡æ–°è®­ç»ƒ
- ä½œä¸ºBERTæ£€æµ‹å™¨çš„è¡¥å……

**é¢„æœŸæ•ˆæœï¼š**
- å¯¹æœªè§æ¨¡å‹è¾¾åˆ°70-80%å‡†ç¡®ç‡
- ç»“åˆBERTå¯æå‡æ•´ä½“é²æ£’æ€§

**æ—¶é—´æˆæœ¬ï¼š** 4-6å°æ—¶ï¼ˆå®ç°+å®éªŒï¼‰

---

#### Task 4.2ï¼šæ°´å°æ£€æµ‹é›†æˆ â­â­
**åŸºäºç ”ç©¶ï¼šSynthID-Textï¼ˆ2025ï¼‰**

**æ¦‚å¿µï¼š**
- æŸäº›AIæ¨¡å‹çš„è¾“å‡ºå¯èƒ½åŒ…å«éšå½¢æ°´å°
- å¯ä»¥ä½œä¸ºè¾…åŠ©æ£€æµ‹æ‰‹æ®µ

**å®æ–½ï¼ˆå¦‚æœæœ‰è®¿é—®æƒé™ï¼‰ï¼š**
```python
# æ£€æµ‹å¸¸è§æ°´å°ç±»å‹
class WatermarkDetector:
    def detect_synthid(self, text):
        """æ£€æµ‹Google SynthIDæ°´å°"""
        # éœ€è¦Google API
        pass

    def detect_openai_watermark(self, text):
        """æ£€æµ‹OpenAIæ°´å°"""
        # éœ€è¦ç›¸å…³åº“
        pass
```

**æ³¨æ„ï¼š**
- å¤§éƒ¨åˆ†å¼€æºæ¨¡å‹æ²¡æœ‰æ°´å°
- ä»…ä½œä¸ºè¡¥å……æ‰‹æ®µ
- ä¸åº”ä½œä¸ºä¸»è¦æ£€æµ‹æ–¹æ³•

**æ—¶é—´æˆæœ¬ï¼š** 2-3å°æ—¶ï¼ˆè°ƒç ”+POCï¼‰

---

## ğŸ“Š æ‰§è¡Œä¼˜å…ˆçº§çŸ©é˜µ

| ä»»åŠ¡ | é‡è¦æ€§ | ç´§æ€¥æ€§ | éš¾åº¦ | æ—¶é—´ | æ¨èé¡ºåº |
|------|--------|--------|------|------|---------|
| Task 1.1 çœŸå®æ•°æ®é‡è®­ | â­â­â­â­â­ | é«˜ | ä½ | 3-4h | **1** |
| Task 3.1 æ ¼å¼å¯¹æŠ—æµ‹è¯• | â­â­â­â­â­ | é«˜ | ä½ | 0.5h | **2** |
| Task 1.2 å¤šæ¨¡å‹æ•°æ® | â­â­â­â­ | ä¸­ | ä¸­ | 5-8h | **3** |
| Task 2.1 å¯¹æŠ—è®­ç»ƒ | â­â­â­â­ | ä¸­ | ä¸­ | 3-4h | **4** |
| Task 3.2 è·¨åŸŸè¯„ä¼° | â­â­â­â­ | ä¸­ | ä¸­ | 3-4h | **5** |
| Task 3.3 å¯¹æŠ—æ”»å‡»æµ‹è¯• | â­â­â­â­ | ä¸­ | ä¸­ | 2-3h | **6** |
| Task 1.3 æ•°æ®å¢å¼º | â­â­â­ | ä½ | ä¸­ | 4-6h | 7 |
| Task 2.2 é›†æˆå­¦ä¹  | â­â­â­ | ä½ | é«˜ | 6-8h | 8 |
| Task 4.1 é›¶æ ·æœ¬æ£€æµ‹ | â­â­â­ | ä½ | é«˜ | 4-6h | 9 |
| Task 4.2 æ°´å°æ£€æµ‹ | â­â­ | ä½ | ä¸­ | 2-3h | 10 |

---

## ğŸ¯ å¿«é€Ÿè¡ŒåŠ¨æ–¹æ¡ˆï¼ˆ3å¤©è®¡åˆ’ï¼‰

### Day 1ï¼šæ•°æ®å’ŒåŸºç¡€æ”¹è¿›ï¼ˆä¼˜å…ˆçº§P0ï¼‰
**ä¸Šåˆï¼ˆ3å°æ—¶ï¼‰ï¼š**
- [x] Task 1.1ï¼šä½¿ç”¨çœŸå®THUCNewsæ•°æ®é‡æ–°è®­ç»ƒ
- [x] éªŒè¯æ–°æ¨¡å‹æ€§èƒ½

**ä¸‹åˆï¼ˆ2å°æ—¶ï¼‰ï¼š**
- [x] Task 3.1ï¼šæ ¼å¼å¯¹æŠ—æµ‹è¯•
- [x] åˆ†æç»“æœï¼Œç¡®è®¤æ ¼å¼å…ç–«æ€§

**æˆæœï¼š**
- åŸºäºçœŸå®äººç±»æ•°æ®çš„æ–°æ¨¡å‹
- æ ¼å¼åå·®éªŒè¯æŠ¥å‘Š

---

### Day 2ï¼šæ‰©å±•æ•°æ®å’Œè¯„ä¼°ï¼ˆä¼˜å…ˆçº§P1ï¼‰
**ä¸Šåˆï¼ˆ4å°æ—¶ï¼‰ï¼š**
- [x] Task 1.2ï¼šæ”¶é›†2-3ä¸ªæ–°æ¨¡å‹çš„æ•°æ®ï¼ˆå„1000æ¡ï¼‰
- [x] é‡æ–°è®­ç»ƒåŒ…å«å¤šæ¨¡å‹æ•°æ®çš„ç‰ˆæœ¬

**ä¸‹åˆï¼ˆ3å°æ—¶ï¼‰ï¼š**
- [x] Task 3.2ï¼šè·¨åŸŸè¯„ä¼°
- [x] Task 3.3ï¼šå¯¹æŠ—æ”»å‡»æµ‹è¯•

**æˆæœï¼š**
- å¤šæ¨¡å‹æ³›åŒ–çš„æ”¹è¿›ç‰ˆ
- å®Œæ•´çš„é²æ£’æ€§è¯„ä¼°æŠ¥å‘Š

---

### Day 3ï¼šé«˜çº§æ”¹è¿›ï¼ˆå¯é€‰ï¼Œä¼˜å…ˆçº§P2ï¼‰
**ä¸Šåˆï¼ˆ3å°æ—¶ï¼‰ï¼š**
- [x] Task 2.1ï¼šå®ç°å¯¹æŠ—è®­ç»ƒæ¡†æ¶
- [x] è®­ç»ƒå¯¹æŠ—é²æ£’æ¨¡å‹

**ä¸‹åˆï¼ˆ4å°æ—¶ï¼‰ï¼š**
- [x] æ•´ç†æ‰€æœ‰å®éªŒç»“æœ
- [x] æ’°å†™è®ºæ–‡çš„å®éªŒç« èŠ‚
- [x] å‡†å¤‡ç­”è¾©ææ–™

**æˆæœï¼š**
- å¯¹æŠ—é²æ£’çš„æœ€ç»ˆæ¨¡å‹
- å®Œæ•´çš„è®ºæ–‡å®éªŒéƒ¨åˆ†

---

## ğŸ“ˆ é¢„æœŸæ”¹è¿›æ•ˆæœ

### åŸºçº¿æ¨¡å‹ï¼ˆå½“å‰ï¼‰
- æµ‹è¯•å‡†ç¡®ç‡ï¼š100%
- è·¨åŸŸèƒ½åŠ›ï¼šæœªçŸ¥
- å¯¹æŠ—é²æ£’æ€§ï¼šæœªçŸ¥
- å¤šæ¨¡å‹æ³›åŒ–ï¼šå¼±ï¼ˆä»…2ä¸ªæ¨¡å‹ï¼‰

### æ”¹è¿›åæ¨¡å‹ï¼ˆé¢„æœŸï¼‰
- æµ‹è¯•å‡†ç¡®ç‡ï¼š85-95%ï¼ˆä½¿ç”¨çœŸå®æ•°æ®ï¼‰
- è·¨åŸŸèƒ½åŠ›ï¼šå„é¢†åŸŸå‡†ç¡®ç‡>80%ï¼Œæ–¹å·®<0.05
- å¯¹æŠ—é²æ£’æ€§ï¼š
  - åŒä¹‰è¯æ›¿æ¢ï¼ˆ10%ï¼‰ï¼šä¸‹é™<5%
  - Back-translationï¼šä¸‹é™<10%
  - LLMæ”¹å†™ï¼šä¸‹é™<15%
- å¤šæ¨¡å‹æ³›åŒ–ï¼šåŒ…å«8-10ä¸ªæ¨¡å‹çš„æ•°æ®
- æ ¼å¼å…ç–«æ€§ï¼šâ­â­â­â­â­ ä¼˜ç§€

### è®ºæ–‡è´¡çŒ®ç‚¹ï¼ˆæ–°å¢ï¼‰
1. **çœŸå®æ•°æ®éªŒè¯**ï¼šä½¿ç”¨THUCNewsçœŸå®æ–°é—»vs AIç”Ÿæˆ
2. **æ ¼å¼åå·®ç ”ç©¶**ï¼šé¦–æ¬¡ç³»ç»Ÿåˆ†æä¸­æ–‡AIæ£€æµ‹çš„æ ¼å¼åå·®
3. **å¤šæ¨¡å‹æ³›åŒ–**ï¼šè·¨8-10ä¸ªä¸»æµLLMçš„æ£€æµ‹èƒ½åŠ›
4. **å¯¹æŠ—é²æ£’æ€§**ï¼š5ç§æ”»å‡»åœºæ™¯çš„å®Œæ•´è¯„ä¼°
5. **è·¨åŸŸæ³›åŒ–**ï¼š5ä¸ªä¸åŒé¢†åŸŸçš„ç¨³å®šè¡¨ç°

---

## ğŸ”§ å®ç”¨å·¥å…·è„šæœ¬

### å¿«é€Ÿå¯åŠ¨è„šæœ¬
```bash
#!/bin/bash
# quick_improve.sh - å¿«é€Ÿæ‰§è¡Œæ ¸å¿ƒæ”¹è¿›

echo "=== Phase 1: çœŸå®æ•°æ®é‡è®­ ==="
python scripts/bert_prep/label_and_merge.py \
  --ai-data datasets/final/parallel_dataset_cleaned.csv \
  --human-data datasets/human_texts/thucnews_real_human_9000.csv \
  --output datasets/raw/parallel_dataset_real.csv

python scripts/bert_prep/split_dataset.py \
  --input datasets/raw/parallel_dataset_real.csv \
  --output-dir datasets/bert_real_human

python scripts/training/train_bert_improved.py \
  --data-dir datasets/bert_real_human \
  --output-dir models/bert_real_human

echo "=== Phase 2: æ ¼å¼å¯¹æŠ—æµ‹è¯• ==="
python scripts/evaluation/format_adversarial_test.py \
  --model-dir models/bert_real_human/best_model \
  --test-file datasets/bert_real_human/test.csv

echo "âœ… æ ¸å¿ƒæ”¹è¿›å®Œæˆï¼"
```

### æ€§èƒ½å¯¹æ¯”è„šæœ¬
```python
# scripts/evaluation/compare_models.py

def compare_all_models():
    """å¯¹æ¯”æ‰€æœ‰æ¨¡å‹ç‰ˆæœ¬"""
    models = {
        'åŸºçº¿æ¨¡å‹': 'models/bert_improved/best_model',
        'çœŸå®æ•°æ®æ¨¡å‹': 'models/bert_real_human/best_model',
        'å¤šæ¨¡å‹æ¨¡å‹': 'models/bert_multi_model/best_model',
        'å¯¹æŠ—è®­ç»ƒæ¨¡å‹': 'models/bert_adversarial/best_model'
    }

    tests = {
        'æ ‡å‡†æµ‹è¯•': 'datasets/bert_real_human/test.csv',
        'æ ¼å¼å¯¹æŠ—': 'adversarial_format_test',
        'è·¨åŸŸæµ‹è¯•': 'cross_domain_test',
        'å¯¹æŠ—æ”»å‡»': 'adversarial_attack_test'
    }

    results = {}
    for model_name, model_path in models.items():
        model = load_model(model_path)
        results[model_name] = {}

        for test_name, test_data in tests.items():
            accuracy = evaluate(model, test_data)
            results[model_name][test_name] = accuracy

    # ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼
    generate_comparison_table(results)
    return results
```

---

## ğŸ“š å‚è€ƒèµ„æº

### 2025å¹´å…³é”®è®ºæ–‡
1. **Sci-SpanDet**: Span-level Detection via Contrastive Learning (AUROC 92.63%)
2. **RAID Benchmark**: 600ä¸‡æ–‡æœ¬ï¼Œ11æ¨¡å‹ï¼Œ8åŸŸï¼Œ11å¯¹æŠ—æ”»å‡»
3. **PIFEæ¡†æ¶**: å¯¹æŠ—é²æ£’æ€§ä»48.8%æå‡è‡³82.6%
4. **GECScore**: é›¶æ ·æœ¬æ£€æµ‹98.62% AUROC
5. **Adversarial Paraphrasing**: æ£€æµ‹ç‡ä¸‹é™87.88%çš„æ”»å‡»æ–¹æ³•

### å®ç”¨å·¥å…·
- Hugging Face Transformers
- TextAttackï¼ˆå¯¹æŠ—æ”»å‡»åº“ï¼‰
- NLTK / SpaCyï¼ˆæ–‡æœ¬å¤„ç†ï¼‰
- Sentence-Transformersï¼ˆè¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰

---

## ğŸ’¡ å…³é”®æ´å¯Ÿ

1. **çœŸå®æ•°æ®è‡³å…³é‡è¦**
   - AIç”Ÿæˆçš„"äººç±»é£æ ¼"æ•°æ®ä¼šå¼•å…¥åå·®
   - THUCNewsçœŸå®æ–°é—»æ˜¯æœ€ä½³é€‰æ‹©

2. **å¤šæ¨¡å‹æ•°æ®æ˜¯æ³›åŒ–çš„å…³é”®**
   - è®­ç»ƒæ•°æ®è‡³å°‘åŒ…å«5-8ä¸ªä¸åŒæ¨¡å‹
   - é¿å…è¿‡æ‹Ÿåˆç‰¹å®šæ¨¡å‹çš„ç‰¹å¾

3. **å¯¹æŠ—é²æ£’æ€§éœ€è¦ä¸“é—¨è®­ç»ƒ**
   - æ ‡å‡†è®­ç»ƒä¸è¶³ä»¥æŠµæŠ—æ”»å‡»
   - å¯¹æŠ—è®­ç»ƒå¯å°†é²æ£’æ€§æå‡70%+

4. **æ ¼å¼åå·®å¿…é¡»æ¶ˆé™¤**
   - Markdownæ ¼å¼æ˜¯å¼ºä¿¡å·
   - å»ååå‡†ç¡®ç‡ä¸‹é™æ˜¯æ­£å¸¸ä¸”ç†æƒ³çš„

5. **è¯„ä¼°æ¯”å•ä¸€å‡†ç¡®ç‡æ›´é‡è¦**
   - è·¨åŸŸæ³›åŒ–ã€å¯¹æŠ—é²æ£’æ€§ã€æ ¼å¼å…ç–«æ€§
   - 85%ç¨³å®š > 99%è„†å¼±

---

## âœ… æˆåŠŸæ ‡å‡†

### å¿…é¡»è¾¾æˆï¼ˆP0ï¼‰
- [x] ä½¿ç”¨çœŸå®äººç±»æ•°æ®é‡æ–°è®­ç»ƒ
- [x] æ ¼å¼å¯¹æŠ—æµ‹è¯•é€šè¿‡ï¼ˆä¸‹é™<10%ï¼‰
- [x] æµ‹è¯•å‡†ç¡®ç‡>85%

### åº”è¯¥è¾¾æˆï¼ˆP1ï¼‰
- [ ] åŒ…å«5-8ä¸ªæ¨¡å‹çš„è®­ç»ƒæ•°æ®
- [ ] è·¨åŸŸå‡†ç¡®ç‡>80%ï¼Œæ–¹å·®<0.05
- [ ] å¯¹æŠ—æ”»å‡»æµ‹è¯•ï¼šåŒä¹‰è¯æ›¿æ¢ä¸‹é™<5%

### å¯ä»¥è¾¾æˆï¼ˆP2ï¼‰
- [ ] å¯¹æŠ—è®­ç»ƒæ¨¡å‹é²æ£’æ€§>80%
- [ ] é›†æˆå­¦ä¹ å‡†ç¡®ç‡æå‡2-5%
- [ ] é›¶æ ·æœ¬æ£€æµ‹POC

---

**æœ€åæ›´æ–°ï¼š** 2026-01-11
**çŠ¶æ€ï¼š** ğŸ“‹ è®¡åˆ’å®Œæˆï¼Œå¾…æ‰§è¡Œ
**é¢„è®¡æ€»æ—¶é—´ï¼š** 20-35å°æ—¶ï¼ˆæ ¹æ®é€‰æ‹©çš„ä»»åŠ¡ï¼‰
