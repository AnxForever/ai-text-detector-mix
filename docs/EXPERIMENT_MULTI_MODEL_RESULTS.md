# 多模型对比实验结果

> AI文本检测 - 多架构对比研究
> 完成日期：2026-01-25
> 数据集：55,438条中文样本（清洗后）

---

## 一、实验背景

### 数据集

在前期实验基础上，完成了数据清洗：
- 删除垃圾模板数据（4,303条"generated_human_style"）
- 合并高质量AI数据：parallel_dataset + AUTO_COMBINED
- 新增HC3-Chinese数据集

| 类别 | 样本数 | 来源 |
|------|--------|------|
| AI文本 | 28,250 | GPT-4, Claude, DeepSeek, ChatGPT |
| 人类文本 | 27,188 | THUCNews + HC3人类回答 |
| **总计** | **55,438** | - |

### 数据划分

| 集合 | 样本数 | 比例 |
|------|--------|------|
| 训练集 | 44,350 | 80% |
| 验证集 | 5,544 | 10% |
| 测试集 | 5,544 | 10% |

---

## 二、模型架构

### 1. BERT V2（基线）
- 架构：bert-base-chinese + Linear(768→2)
- 参数：~110M
- 特点：纯BERT分类，无额外结构

### 2. BERT-BiGRU
- 架构：BERT(768) → BiGRU(256×2) → Attention → FC(2)
- 参数：~115M
- 特点：双向GRU捕获序列依赖，注意力机制聚合

### 3. DPCNN
- 架构：Embedding → Conv1D → 残差金字塔池化
- 参数：~5M
- 特点：轻量级，深度金字塔结构

### 4. BERT-Graph
- 架构：BERT → Self-Attention Graph → FC(2)
- 参数：~112M
- 特点：自注意力模拟图结构，捕获token间关系

---

## 三、训练配置

| 参数 | BERT系列 | DPCNN |
|------|----------|-------|
| Batch Size | 8 | 32 |
| Learning Rate | 2e-5 | 1e-3 |
| Epochs | 3 | 10 |
| Max Length | 512 | 512 |
| Optimizer | AdamW | Adam |
| GPU | 8GB | 8GB |

---

## 四、实验结果

### 准确率对比

| 模型 | 测试准确率 | 验证准确率 | AUC | 训练时间 |
|------|-----------|-----------|-----|----------|
| **BERT V2** | **99.26%** | - | **0.9998** | ~2h |
| BERT-Graph | 99.21% | 99.19% | - | ~3h |
| BERT-BiGRU | 98.97% | 99.06% | - | ~2.5h |
| DPCNN | 97.96% | 98.25% | - | ~30min |

### BERT V2 混淆矩阵

|  | 预测Human | 预测AI |
|--|----------|--------|
| 实际Human | 2683 | 36 |
| 实际AI | 5 | 2820 |

- 误报（Human→AI）：36例（1.3%）
- 漏报（AI→Human）：5例（0.2%）

### 结果分析

1. **BERT V2表现最佳**（99.26%）
   - 简单架构反而最优
   - 预训练知识充分利用

2. **BERT-Graph次之**（99.21%）
   - 图结构带来微小提升（相比BiGRU）
   - 计算开销增加

3. **BERT-BiGRU**（98.97%）
   - BiGRU增加了序列建模
   - 但对分类任务提升有限

4. **DPCNN最轻量**（97.96%）
   - 参数量仅5M（BERT的1/20）
   - 准确率仍达97.96%
   - 适合资源受限场景

---

## 五、结论

1. **所有模型均达到>97%准确率**，验证了数据集质量
2. **简单BERT架构最优**，复杂结构未带来显著提升
3. **DPCNN性价比最高**，适合部署场景
4. **数据清洗至关重要**，删除垃圾数据后模型表现稳定

---

## 六、模型文件

```
models/
├── bert_v2/          # 99.26% ⭐ 最佳
├── bert_graph/       # 99.21%
├── bert_bigru/       # 98.97%
└── dpcnn/            # 97.96%
```

---

## 七、下一步

- [ ] ROC-AUC曲线对比
- [ ] 混淆矩阵分析
- [ ] 错误样本分析
- [ ] LoRA微调大模型（Qwen-7B）
