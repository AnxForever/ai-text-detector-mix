# AI文本检测研究问题清单

> 提交给Gemini深度研究
> 背景：当前数据集（55K中文样本）模型准确率99%+，但混合文本准确率仅50%，说明数据过于简单

---

## 一、困难数据集构建

### 1.1 混合文本（Human-AI Hybrid）

**现状**：人类开头+AI续写的混合文本，模型准确率降至50%

**待研究**：
- 混合文本应该标注为AI还是Human？学术界有无共识？
- 混合比例（如30%人类+70%AI）如何影响检测难度？
- 是否需要引入"混合"作为第三类标签？
- 有哪些公开的混合文本数据集？

### 1.2 人类化AI文本

**待研究**：
- 如何通过prompt engineering让AI生成更像人类的文本？
- 有哪些已知的"AI去特征化"技术（paraphrasing attacks）？
- 学术界如何构建对抗样本来测试检测器鲁棒性？
- DIPPER、PEGASUS等改写模型的效果如何？

### 1.3 AI辅助人类写作

**待研究**：
- 人类使用AI建议/补全后的文本如何标注？
- Grammarly、Copilot等工具辅助的文本算AI吗？
- 这类数据如何收集？

### 1.4 多轮对话与长文本

**待研究**：
- 长文本（>2000字）的检测难度是否更高？
- 多轮对话中AI回复的检测有何特殊性？
- 是否需要分段检测？

---

## 二、数据质量评价体系

### 2.1 难度量化指标

**核心问题**：如何量化一个样本的"检测难度"？

**待研究**：
- 基于模型置信度的难度评估（低置信度=高难度）
- 基于特征重叠度的难度评估（AI/Human特征越相似越难）
- 基于人类标注一致性的难度评估（人类也分不清=高难度）
- 有无现成的难度评分公式或框架？

### 2.2 数据集多样性指标

**待研究**：
- 如何衡量数据集的领域覆盖度？
- 如何衡量AI来源多样性（GPT/Claude/Gemini/国产模型）？
- 如何衡量文本风格多样性？
- 有无标准的数据集多样性评估工具？

### 2.3 偏差检测指标

**待研究**：
- 如何检测数据集中的格式偏差（如markdown）？
- 如何检测长度偏差（AI文本普遍更长）？
- 如何检测主题偏差？
- 有无自动化的偏差检测工具？

### 2.4 数据集质量综合评分

**待研究**：
- 是否有类似ImageNet的AI文本检测benchmark？
- 学术界如何评价一个AI检测数据集的质量？
- 能否设计一个综合评分公式：`Quality = f(难度, 多样性, 平衡性, 规模)`？

---

## 三、训练策略优化

### 3.1 课程学习（Curriculum Learning）

**待研究**：
- 先易后难的训练策略是否有效？
- 如何自动排序样本难度？
- 有无AI检测领域的课程学习实践？

### 3.2 对抗训练

**待研究**：
- 如何生成对抗样本来增强模型鲁棒性？
- 对抗训练在AI检测领域的最新进展？
- 与GAN结合的检测器效果如何？

### 3.3 数据增强

**待研究**：
- 回译（back-translation）对AI检测的影响？
- 同义词替换是否会破坏AI特征？
- 有哪些适合中文的数据增强方法？

### 3.4 主动学习

**待研究**：
- 如何选择最有价值的样本进行标注？
- 不确定性采样在AI检测中的应用？

---

## 四、评估方法论

### 4.1 跨域泛化测试

**待研究**：
- 如何设计跨领域测试集（训练用新闻，测试用论文）？
- 如何设计跨模型测试集（训练用GPT，测试用Claude）？
- 跨时间测试（用旧模型数据训练，测试新模型）？

### 4.2 鲁棒性测试

**待研究**：
- 标准的鲁棒性测试协议是什么？
- 如何测试对改写攻击的抵抗力？
- 如何测试对翻译攻击的抵抗力？

### 4.3 人类基线

**待研究**：
- 人类在AI检测任务上的准确率是多少？
- 如何设计人类评估实验？
- 人类和模型的错误模式有何不同？

---

## 五、具体研究问题（优先级排序）

### P0 - 最急需解答

1. **混合文本的标注标准**：学术界对Human-AI混合文本如何定义和标注？
2. **难度量化公式**：有无现成的样本难度评分方法？
3. **中文AI检测benchmark**：有哪些高质量的中文AI检测数据集和评测基准？

### P1 - 重要

4. **对抗样本生成**：如何系统性地生成能欺骗检测器的AI文本？
5. **数据集质量评估框架**：如何全面评估一个AI检测数据集的质量？
6. **课程学习实践**：AI检测领域有无成功的课程学习案例？

### P2 - 有价值

7. **跨模型泛化**：如何提高模型对未见过的AI模型的检测能力？
8. **长文本检测**：长文本是否需要特殊处理？
9. **实时检测**：如何在用户输入时实时检测？

---

## 六、期望输出

请Gemini针对以上问题：

1. **综述现有研究**：每个问题的学术界现状
2. **推荐论文**：每个方向的代表性论文（最好有中文相关）
3. **实践建议**：可落地的具体方案
4. **开源资源**：相关的开源数据集、代码、工具

---

## 附录：当前项目状态

- 数据集：55,438条中文样本（28K AI + 27K Human）
- 模型：BERT-base-chinese，准确率99.26%
- 弱点：混合文本准确率50%
- 目标：CCF-C/中文核心期刊发表
