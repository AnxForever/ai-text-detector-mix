# è§£å†³é•¿åº¦åå·®é—®é¢˜ - å¿«é€ŸæŒ‡å—

> é’ˆå¯¹CodexæŠ¥å‘Šçš„AI(1680) vs äººç±»(942)é•¿åº¦å·®å¼‚é—®é¢˜

---

## ğŸš¨ é—®é¢˜åˆ†æ

### CodexæŠ¥å‘Šçš„é—®é¢˜
```
AIå¹³å‡é•¿åº¦: 1680å­—ç¬¦
äººç±»å¹³å‡é•¿åº¦: 942å­—ç¬¦
å·®è·: 78%

3000+å­—ç¬¦åŒºé—´: å‡ ä¹åªæœ‰AI
```

### ä¸ºä»€ä¹ˆè¿™æ˜¯é—®é¢˜ï¼Ÿ
æ¨¡å‹å¯èƒ½å­¦åˆ°**ç®€å•è§„åˆ™**ï¼š
- âŒ "é•¿æ–‡æœ¬ = AI"
- âŒ "çŸ­æ–‡æœ¬ = äººç±»"

è€Œä¸æ˜¯å­¦ä¹ **çœŸæ­£çš„è¯­ä¹‰ç‰¹å¾**ï¼š
- âœ… è¯æ±‡ä½¿ç”¨æ¨¡å¼
- âœ… å¥æ³•ç»“æ„
- âœ… é€»è¾‘è¿è´¯æ€§

### åæœ
- æ³›åŒ–èƒ½åŠ›å·®
- å¯¹é•¿åº¦æ‰°åŠ¨æ•æ„Ÿ
- æ— æ³•æ£€æµ‹"çŸ­AIæ–‡æœ¬"æˆ–"é•¿äººç±»æ–‡æœ¬"

---

## âœ… è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ1ï¼šé•¿åº¦å¹³è¡¡ï¼ˆæ¨èï¼‰

**ç­–ç•¥**ï¼š
1. æˆªæ–­è¿‡é•¿AIæ–‡æœ¬ï¼ˆ>2500å­—ç¬¦ï¼‰
2. è¿‡æ»¤è¿‡çŸ­äººç±»æ–‡æœ¬ï¼ˆ<300å­—ç¬¦ï¼‰
3. æŒ‰é•¿åº¦åˆ†å±‚é‡‡æ ·ï¼Œä½¿åˆ†å¸ƒæ¥è¿‘

**ä½¿ç”¨**ï¼š
```bash
python scripts/data_cleaning/balance_length_distribution.py
```

**æ•ˆæœ**ï¼š
```
åŸå§‹:
  AI: 1680å­—ç¬¦
  äººç±»: 942å­—ç¬¦
  å·®è·: 78%

å¹³è¡¡å:
  AI: ~1200å­—ç¬¦
  äººç±»: ~1100å­—ç¬¦
  å·®è·: <10%
```

---

### æ–¹æ¡ˆ2ï¼šé•¿åº¦åŠ æƒè®­ç»ƒ

**ç­–ç•¥**ï¼šåœ¨è®­ç»ƒæ—¶é™ä½é•¿åº¦ç‰¹å¾çš„æƒé‡

**å®ç°**ï¼š
```python
# åœ¨æŸå¤±å‡½æ•°ä¸­åŠ å…¥é•¿åº¦æƒ©ç½š
loss = criterion(logits, labels)
length_penalty = abs(length - target_length) / target_length
loss = loss * (1 + 0.1 * length_penalty)
```

---

### æ–¹æ¡ˆ3ï¼šæ•°æ®å¢å¼º

**ç­–ç•¥**ï¼š
- æˆªæ–­é•¿æ–‡æœ¬ç”Ÿæˆå¤šä¸ªæ ·æœ¬
- æ‹¼æ¥çŸ­æ–‡æœ¬

**ç¤ºä¾‹**ï¼š
```python
# é•¿æ–‡æœ¬ -> å¤šä¸ªçŸ­æ ·æœ¬
long_text = "..." # 3000å­—ç¬¦
samples = [
    long_text[:1000],
    long_text[500:1500],
    long_text[1000:2000]
]
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### é€‰é¡¹Aï¼šä¸€é”®è¿è¡Œï¼ˆæ¨èï¼‰
```bash
python run_length_balanced_training.py
```

è¿™ä¸ªè„šæœ¬ä¼šï¼š
1. âœ… è‡ªåŠ¨å¹³è¡¡é•¿åº¦åˆ†å¸ƒ
2. âœ… é‡æ–°åˆ’åˆ†æ•°æ®é›†
3. âœ… å¯é€‰æ ¼å¼å»å
4. âœ… è®­ç»ƒæ–°æ¨¡å‹

---

### é€‰é¡¹Bï¼šæ‰‹åŠ¨æ­¥éª¤

#### æ­¥éª¤1ï¼šé•¿åº¦å¹³è¡¡
```bash
python scripts/data_cleaning/balance_length_distribution.py
```

#### æ­¥éª¤2ï¼šåˆ’åˆ†æ•°æ®é›†
```bash
python scripts/bert_prep/split_dataset.py \
  --input datasets/bert/full_dataset_length_balanced.csv \
  --output_dir datasets/bert_balanced
```

#### æ­¥éª¤3ï¼šè®­ç»ƒ
```bash
# ä»å¤´è®­ç»ƒ
python scripts/training/train_bert_improved.py \
  --train_csv datasets/bert_balanced/train.csv \
  --val_csv datasets/bert_balanced/val.csv \
  --test_csv datasets/bert_balanced/test.csv \
  --batch_size 8 \
  --epochs 5

# æˆ–ä»å·²æœ‰æ¨¡å‹ç»§ç»­è®­ç»ƒ
python scripts/training/continue_training.py \
  --base_model models/bert_improved/best_model \
  --output_dir models/bert_improved_v2 \
  --train_data datasets/bert_balanced/train.csv \
  --batch_size 8 \
  --lr 1e-5
```

---

## ğŸ“Š è¯„ä¼°é•¿åº¦åå·®

### ä½¿ç”¨é•¿åº¦æ„ŸçŸ¥è¯„ä¼°
```bash
python scripts/evaluation/length_aware_evaluation.py \
  --model_path models/bert_improved_v2/best_model \
  --test_data datasets/bert_balanced/test.csv
```

### é¢„æœŸè¾“å‡º
```
é•¿åº¦åŒºé—´æ€§èƒ½:
  300-500:   å‡†ç¡®ç‡ 98.5%
  500-1000:  å‡†ç¡®ç‡ 99.2%
  1000-1500: å‡†ç¡®ç‡ 99.5%
  1500-2000: å‡†ç¡®ç‡ 99.3%
  2000-2500: å‡†ç¡®ç‡ 98.8%

âœ“ å„åŒºé—´æ€§èƒ½å‡è¡¡ï¼Œæ— æ˜æ˜¾é•¿åº¦åå·®
```

---

## ğŸ”¬ å¯¹æ¯”å®éªŒ

### å®éªŒè®¾è®¡
| æ¨¡å‹ | æ•°æ® | å¹³å‡é•¿åº¦å·® | å‡†ç¡®ç‡ | é•¿åº¦æ•æ„Ÿåº¦ |
|------|------|-----------|--------|-----------|
| æ¨¡å‹A | åŸå§‹æ•°æ® | 78% | 100% | é«˜ |
| æ¨¡å‹B | é•¿åº¦å¹³è¡¡ | <10% | 99.5% | ä½ |

### é•¿åº¦æ•æ„Ÿåº¦æµ‹è¯•
```python
# æµ‹è¯•ï¼šå°†AIæ–‡æœ¬æˆªæ–­åˆ°500å­—ç¬¦
original_acc = 100%
truncated_acc = ?

# æœŸæœ›ï¼š
# æ¨¡å‹Aï¼ˆæœªå¹³è¡¡ï¼‰: å‡†ç¡®ç‡å¤§å¹…ä¸‹é™ (100% -> 85%)
# æ¨¡å‹Bï¼ˆå·²å¹³è¡¡ï¼‰: å‡†ç¡®ç‡ç¨³å®š (99.5% -> 98%)
```

---

## ğŸ’¡ å…³é”®å»ºè®®

### 1. ä¼˜å…ˆå¤„ç†é•¿åº¦åå·®
åœ¨å®ç°å¤æ‚ç‰¹å¾ï¼ˆç»Ÿè®¡ã€å›¾ï¼‰ä¹‹å‰ï¼Œå…ˆè§£å†³é•¿åº¦åå·®ã€‚

### 2. ä¿ç•™åŸå§‹æ¨¡å‹
```bash
# ä¸è¦è¦†ç›–å·²æœ‰çš„100%å‡†ç¡®ç‡æ¨¡å‹
--output_dir models/bert_improved_v2  # æ–°ç›®å½•
```

### 3. å¯¹æ¯”è¯„ä¼°
è®­ç»ƒåå¯¹æ¯”ï¼š
- åŸå§‹æ¨¡å‹ vs é•¿åº¦å¹³è¡¡æ¨¡å‹
- å„é•¿åº¦åŒºé—´çš„æ€§èƒ½
- å¯¹é•¿åº¦æ‰°åŠ¨çš„é²æ£’æ€§

### 4. è®ºæ–‡ä¸­çš„å¤„ç†
```latex
\subsection{Addressing Length Bias}

We observed a significant length discrepancy between AI (1680 chars) 
and human texts (942 chars). To prevent the model from learning 
spurious length-based patterns, we applied length balancing:

1. Truncate AI texts exceeding 2500 characters
2. Filter human texts shorter than 300 characters  
3. Stratified sampling to equalize length distributions

After balancing, the average length difference reduced from 78% to 
less than 10%, ensuring the model learns semantic features rather 
than length patterns.
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. æ˜¾å­˜é™åˆ¶
```bash
# 8GBæ˜¾å­˜å»ºè®®é…ç½®
--batch_size 8
--max_length 512
```

### 2. å­¦ä¹ ç‡
```bash
# ç»§ç»­è®­ç»ƒç”¨æ›´å°çš„å­¦ä¹ ç‡
--lr 1e-5  # è€Œä¸æ˜¯ 2e-5
```

### 3. æ•°æ®å¤‡ä»½
```bash
# å¤„ç†å‰å¤‡ä»½åŸå§‹æ•°æ®
cp datasets/bert/full_dataset_labeled.csv \
   datasets/bert/full_dataset_labeled_backup.csv
```

---

## ğŸ“ æ£€æŸ¥æ¸…å•

- [ ] è¿è¡Œé•¿åº¦å¹³è¡¡è„šæœ¬
- [ ] æ£€æŸ¥å¹³è¡¡åçš„é•¿åº¦åˆ†å¸ƒ
- [ ] é‡æ–°åˆ’åˆ†train/val/test
- [ ] è®­ç»ƒæ–°æ¨¡å‹ï¼ˆä¸è¦†ç›–æ—§æ¨¡å‹ï¼‰
- [ ] è¿è¡Œé•¿åº¦æ„ŸçŸ¥è¯„ä¼°
- [ ] å¯¹æ¯”åŸå§‹æ¨¡å‹ vs å¹³è¡¡æ¨¡å‹
- [ ] æµ‹è¯•é•¿åº¦æ‰°åŠ¨é²æ£’æ€§
- [ ] æ›´æ–°è®ºæ–‡ä¸­çš„æ•°æ®å¤„ç†ç« èŠ‚

---

## ğŸ¯ é¢„æœŸç»“æœ

### æ€§èƒ½å¯¹æ¯”
```
åŸå§‹æ¨¡å‹ï¼ˆé•¿åº¦åå·®ï¼‰:
  âœ“ æµ‹è¯•å‡†ç¡®ç‡: 100%
  âœ— çŸ­AIæ–‡æœ¬: 85%
  âœ— é•¿äººç±»æ–‡æœ¬: 80%
  âœ— é•¿åº¦æ•æ„Ÿåº¦: é«˜

é•¿åº¦å¹³è¡¡æ¨¡å‹:
  âœ“ æµ‹è¯•å‡†ç¡®ç‡: 99.5%
  âœ“ çŸ­AIæ–‡æœ¬: 98%
  âœ“ é•¿äººç±»æ–‡æœ¬: 97%
  âœ“ é•¿åº¦æ•æ„Ÿåº¦: ä½
  âœ“ æ³›åŒ–èƒ½åŠ›: å¼º
```

### è®ºæ–‡ä»·å€¼
- âœ… å‘ç°å¹¶è§£å†³é•¿åº¦åå·®é—®é¢˜
- âœ… è¯æ˜æ¨¡å‹å­¦ä¹ è¯­ä¹‰è€Œéé•¿åº¦
- âœ… æå‡æ¨¡å‹é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›

---

**æ€»ç»“**ï¼šé•¿åº¦åå·®æ˜¯ä¸ªä¸¥é‡é—®é¢˜ï¼Œä½†å®¹æ˜“è§£å†³ã€‚å¤„ç†åæ¨¡å‹ä¼šæ›´robustï¼Œè®ºæ–‡ä¹Ÿæ›´æœ‰è¯´æœåŠ›ï¼
