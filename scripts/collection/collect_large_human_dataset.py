#!/usr/bin/env python3
"""
å¤§è§„æ¨¡äººç±»æ–‡æœ¬æ•°æ®æ”¶é›†å·¥å…·
ç›®æ ‡ï¼šæ”¶é›†5ä¸‡æ¡é«˜è´¨é‡ä¸­æ–‡äººç±»æ–‡æœ¬
"""

import os
import pandas as pd
from datetime import datetime
from pathlib import Path
from typing import List, Dict

class LargeScaleCollector:
    def __init__(self, output_dir="datasets/human_large"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.min_length = 300
        self.max_length = 3000
        
    def collect_thucnews(self, target=20000):
        """THUCNewsæ–°é—»æ•°æ®é›†ï¼ˆ74ä¸‡ç¯‡ï¼‰"""
        print(f"\n[1/5] THUCNews - ç›®æ ‡: {target}æ¡")
        try:
            from datasets import load_dataset
            ds = load_dataset("oyxy2019/THUCNewsText", split="train")
            df = pd.DataFrame(ds)
            df['length'] = df['text'].str.len()
            df = df[(df['length'] >= self.min_length) & (df['length'] <= self.max_length)]
            df = df.sample(n=min(target, len(df)), random_state=42)
            df['source'] = 'thucnews'
            self._save(df, 'thucnews')
            return df
        except Exception as e:
            print(f"âœ— é”™è¯¯: {e}")
            return pd.DataFrame()
    
    def collect_wikipedia(self, target=10000):
        """Wikipediaä¸­æ–‡ç™¾ç§‘"""
        print(f"\n[2/5] Wikipedia - ç›®æ ‡: {target}æ¡")
        try:
            from datasets import load_dataset
            ds = load_dataset("wikipedia", "20220301.zh", split="train")
            df = pd.DataFrame(ds)
            df['length'] = df['text'].str.len()
            df = df[(df['length'] >= self.min_length) & (df['length'] <= self.max_length)]
            df = df.sample(n=min(target, len(df)), random_state=42)
            df['source'] = 'wikipedia'
            self._save(df, 'wikipedia')
            return df
        except Exception as e:
            print(f"âœ— é”™è¯¯: {e}")
            return pd.DataFrame()
    
    def collect_clue(self, target=10000):
        """CLUEä¸­æ–‡è¯­æ–™"""
        print(f"\n[3/5] CLUE - ç›®æ ‡: {target}æ¡")
        try:
            from datasets import load_dataset
            # CLUEåŒ…å«å¤šä¸ªå­ä»»åŠ¡ï¼Œè¿™é‡Œç”¨TNEWSï¼ˆä»Šæ—¥å¤´æ¡æ–°é—»ï¼‰
            ds = load_dataset("clue", "tnews", split="train")
            df = pd.DataFrame(ds)
            if 'sentence' in df.columns:
                df = df.rename(columns={'sentence': 'text'})
            df['length'] = df['text'].str.len()
            df = df[(df['length'] >= self.min_length) & (df['length'] <= self.max_length)]
            df = df.sample(n=min(target, len(df)), random_state=42)
            df['source'] = 'clue'
            self._save(df, 'clue')
            return df
        except Exception as e:
            print(f"âœ— é”™è¯¯: {e}")
            return pd.DataFrame()
    
    def collect_weibo(self, target=5000):
        """å¾®åšæ–‡æœ¬ï¼ˆå¦‚æœå¯ç”¨ï¼‰"""
        print(f"\n[4/5] Weibo - ç›®æ ‡: {target}æ¡")
        try:
            from datasets import load_dataset
            ds = load_dataset("dirtycomputer/weibo_senti_100k", split="train")
            df = pd.DataFrame(ds)
            if 'review' in df.columns:
                df = df.rename(columns={'review': 'text'})
            df['length'] = df['text'].str.len()
            df = df[(df['length'] >= self.min_length) & (df['length'] <= self.max_length)]
            df = df.sample(n=min(target, len(df)), random_state=42)
            df['source'] = 'weibo'
            self._save(df, 'weibo')
            return df
        except Exception as e:
            print(f"âœ— é”™è¯¯: {e}")
            return pd.DataFrame()
    
    def collect_csl(self, target=5000):
        """ä¸­æ–‡ç§‘å­¦æ–‡çŒ®ï¼ˆCSLï¼‰"""
        print(f"\n[5/5] CSL - ç›®æ ‡: {target}æ¡")
        try:
            from datasets import load_dataset
            ds = load_dataset("neuclir/csl", split="train")
            df = pd.DataFrame(ds)
            if 'abst' in df.columns:
                df = df.rename(columns={'abst': 'text'})
            df['length'] = df['text'].str.len()
            df = df[(df['length'] >= self.min_length) & (df['length'] <= self.max_length)]
            df = df.sample(n=min(target, len(df)), random_state=42)
            df['source'] = 'csl'
            self._save(df, 'csl')
            return df
        except Exception as e:
            print(f"âœ— é”™è¯¯: {e}")
            return pd.DataFrame()
    
    def _save(self, df, name):
        if len(df) == 0:
            return
        output = self.output_dir / f"{name}_{len(df)}.csv"
        df[['text', 'source', 'length']].to_csv(output, index=False, encoding='utf-8-sig')
        print(f"âœ“ ä¿å­˜ {len(df)} æ¡ -> {output}")
        print(f"  å¹³å‡é•¿åº¦: {df['length'].mean():.0f} å­—ç¬¦")
    
    def merge_all(self):
        """åˆå¹¶æ‰€æœ‰æ•°æ®"""
        print(f"\n{'='*60}")
        print("åˆå¹¶æ‰€æœ‰æ•°æ®...")
        print('='*60)
        
        files = list(self.output_dir.glob("*.csv"))
        if not files:
            print("âœ— æ²¡æœ‰æ‰¾åˆ°æ•°æ®æ–‡ä»¶")
            return None
        
        dfs = []
        for f in files:
            if f.name.startswith("merged_"):
                continue
            df = pd.read_csv(f, encoding='utf-8-sig')
            dfs.append(df)
            print(f"  {f.name}: {len(df)} æ¡")
        
        merged = pd.concat(dfs, ignore_index=True)
        merged = merged.drop_duplicates(subset=['text'], keep='first')
        
        output = self.output_dir / f"merged_human_{len(merged)}.csv"
        merged.to_csv(output, index=False, encoding='utf-8-sig')
        
        print(f"\nâœ“ åˆå¹¶å®Œæˆ: {len(merged)} æ¡ï¼ˆå»é‡åï¼‰")
        print(f"  ä¿å­˜åˆ°: {output}")
        print(f"  å¹³å‡é•¿åº¦: {merged['length'].mean():.0f}")
        print(f"  é•¿åº¦èŒƒå›´: {merged['length'].min()} - {merged['length'].max()}")
        print(f"\næ•°æ®æºåˆ†å¸ƒ:")
        print(merged['source'].value_counts())
        
        return merged


def main():
    print("="*60)
    print("å¤§è§„æ¨¡äººç±»æ–‡æœ¬æ•°æ®æ”¶é›†")
    print("ç›®æ ‡: 50,000æ¡é«˜è´¨é‡ä¸­æ–‡æ–‡æœ¬")
    print("="*60)
    
    collector = LargeScaleCollector()
    
    # æ”¶é›†å„æ•°æ®æº
    collector.collect_thucnews(target=20000)
    collector.collect_wikipedia(target=10000)
    collector.collect_clue(target=10000)
    collector.collect_weibo(target=5000)
    collector.collect_csl(target=5000)
    
    # åˆå¹¶
    merged = collector.merge_all()
    
    if merged is not None and len(merged) >= 50000:
        print(f"\nğŸ‰ æˆåŠŸï¼æ”¶é›†äº† {len(merged)} æ¡äººç±»æ–‡æœ¬")
    else:
        print(f"\nâš ï¸  å½“å‰æ”¶é›†äº† {len(merged) if merged is not None else 0} æ¡ï¼Œæœªè¾¾åˆ°5ä¸‡ç›®æ ‡")
        print("   å¯ä»¥è°ƒæ•´å„æ•°æ®æºçš„targetå‚æ•°æˆ–æ·»åŠ æ›´å¤šæ•°æ®æº")


if __name__ == "__main__":
    main()
