"""
æ ¼å¼åå·®éªŒè¯å·¥å…·

åŠŸèƒ½ï¼š
1. è¯„ä¼°æ•°æ®é›†çš„æ ¼å¼åå·®
2. æµ‹è¯•ç®€å•è§„åˆ™çš„å‡†ç¡®ç‡
3. å¯¹æ¯”ç®€å•è§„åˆ™ vs BERTæ¨¡å‹çš„æ€§èƒ½
4. éªŒè¯æ ¼å¼å»åæ•ˆæœ

ä½œè€…ï¼šFormat Debiasing Team
æ—¥æœŸï¼š2026-01-11
"""

import sys
import io
import os
os.environ['PYTHONIOENCODING'] = 'utf-8'

if sys.platform == 'win32':
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
    except:
        pass

import pandas as pd
import argparse
from datetime import datetime
from typing import Dict, Tuple

# å¯¼å…¥æ ¼å¼å¤„ç†å‡½æ•°
sys.path.insert(0, os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'data_cleaning'))
from format_handler import (
    has_markdown,
    has_markdown_detailed,
    get_format_statistics
)


def evaluate_format_bias(df: pd.DataFrame) -> Dict:
    """
    è¯„ä¼°æ•°æ®é›†çš„æ ¼å¼åå·®

    Args:
        df: æ•°æ®æ¡†ï¼ˆåŒ…å« text å’Œ label åˆ—ï¼‰

    Returns:
        æ ¼å¼åå·®è¯„ä¼°ç»“æœ
    """
    ai_df = df[df['label'] == 1]
    human_df = df[df['label'] == 0]

    # ç»Ÿè®¡markdownæ¯”ä¾‹
    ai_md_rate = ai_df['text'].apply(has_markdown).mean()
    human_md_rate = human_df['text'].apply(has_markdown).mean()
    bias = abs(ai_md_rate - human_md_rate)

    # è·å–è¯¦ç»†ç»Ÿè®¡
    ai_stats = get_format_statistics(ai_df['text'].tolist())
    human_stats = get_format_statistics(human_df['text'].tolist())

    # æ¨¡æ‹Ÿç®€å•è§„åˆ™ï¼ˆåªåˆ¤æ–­markdownï¼‰
    df_temp = df.copy()
    df_temp['simple_pred'] = df_temp['text'].apply(lambda x: 1 if has_markdown(x) else 0)
    simple_accuracy = (df_temp['simple_pred'] == df_temp['label']).mean()

    # è®¡ç®—æ··æ·†çŸ©é˜µ
    tp = ((df_temp['simple_pred'] == 1) & (df_temp['label'] == 1)).sum()  # æ­£ç¡®è¯†åˆ«AI
    fp = ((df_temp['simple_pred'] == 1) & (df_temp['label'] == 0)).sum()  # äººç±»è¯¯åˆ¤ä¸ºAI
    tn = ((df_temp['simple_pred'] == 0) & (df_temp['label'] == 0)).sum()  # æ­£ç¡®è¯†åˆ«äººç±»
    fn = ((df_temp['simple_pred'] == 0) & (df_temp['label'] == 1)).sum()  # AIè¯¯åˆ¤ä¸ºäººç±»

    simple_precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    simple_recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    simple_f1 = 2 * (simple_precision * simple_recall) / (simple_precision + simple_recall) \
                if (simple_precision + simple_recall) > 0 else 0

    # åˆ¤æ–­çŠ¶æ€
    if bias < 0.05:
        status = 'pass'
        recommendation = "âœ… æ ¼å¼åå·®<5%ï¼Œåˆ†å¸ƒéå¸¸å¹³è¡¡ï¼"
    elif bias < 0.15:
        status = 'warning'
        recommendation = "âš ï¸ æ ¼å¼åå·®5-15%ï¼ŒåŸºæœ¬å¹³è¡¡ï¼Œä½†å¯ä»¥è¿›ä¸€æ­¥æ”¹å–„"
    else:
        status = 'fail'
        recommendation = "ğŸ”´ æ ¼å¼åå·®>15%ï¼Œæ¨¡å‹å¯èƒ½è¿‡æ‹Ÿåˆæ ¼å¼ç‰¹å¾"

    return {
        'ai_markdown_rate': ai_md_rate,
        'human_markdown_rate': human_md_rate,
        'bias': bias,
        'ai_stats': ai_stats,
        'human_stats': human_stats,
        'simple_rule': {
            'accuracy': simple_accuracy,
            'precision': simple_precision,
            'recall': simple_recall,
            'f1': simple_f1,
            'confusion_matrix': {
                'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn
            }
        },
        'status': status,
        'recommendation': recommendation
    }


def compare_simple_vs_bert(test_df: pd.DataFrame, bert_model=None, tokenizer=None) -> Dict:
    """
    å¯¹æ¯”ç®€å•è§„åˆ™ vs BERTæ¨¡å‹

    Args:
        test_df: æµ‹è¯•æ•°æ®é›†
        bert_model: BERTæ¨¡å‹ï¼ˆå¯é€‰ï¼‰
        tokenizer: Tokenizerï¼ˆå¯é€‰ï¼‰

    Returns:
        å¯¹æ¯”ç»“æœ
    """
    # ç®€å•è§„åˆ™æ€§èƒ½
    test_df_temp = test_df.copy()
    test_df_temp['simple_pred'] = test_df_temp['text'].apply(lambda x: 1 if has_markdown(x) else 0)
    simple_accuracy = (test_df_temp['simple_pred'] == test_df_temp['label']).mean()

    result = {
        'simple_accuracy': simple_accuracy,
        'bert_accuracy': None,
        'improvement': None
    }

    # å¦‚æœæä¾›äº†BERTæ¨¡å‹ï¼Œè¯„ä¼°BERTæ€§èƒ½
    if bert_model is not None and tokenizer is not None:
        import torch

        bert_model.eval()
        predictions = []

        with torch.no_grad():
            for text in test_df['text']:
                encoding = tokenizer(
                    text,
                    max_length=512,
                    padding='max_length',
                    truncation=True,
                    return_tensors='pt'
                )

                outputs = bert_model(**encoding)
                pred = torch.argmax(outputs.logits, dim=1).item()
                predictions.append(pred)

        bert_accuracy = (pd.Series(predictions) == test_df['label']).mean()

        result['bert_accuracy'] = bert_accuracy
        result['improvement'] = bert_accuracy - simple_accuracy

    return result


def print_format_bias_report(results: Dict, dataset_name: str = ""):
    """æ‰“å°æ ¼å¼åå·®æŠ¥å‘Š"""

    print("\n" + "="*70)
    if dataset_name:
        print(f"æ ¼å¼åå·®è¯„ä¼°æŠ¥å‘Š - {dataset_name}")
    else:
        print("æ ¼å¼åå·®è¯„ä¼°æŠ¥å‘Š")
    print("="*70)

    # åŸºæœ¬ç»Ÿè®¡
    print(f"\nã€æ ¼å¼åˆ†å¸ƒç»Ÿè®¡ã€‘")
    print(f"AIæ–‡æœ¬ markdownç‡: {results['ai_markdown_rate']*100:.2f}%")
    print(f"äººç±»æ–‡æœ¬ markdownç‡: {results['human_markdown_rate']*100:.2f}%")
    print(f"æ ¼å¼åå·®: {results['bias']*100:.2f}%")

    print(f"\nçŠ¶æ€: {results['recommendation']}")

    # è¯¦ç»†æ ¼å¼ç±»å‹ç»Ÿè®¡
    print(f"\nã€æ ¼å¼ç±»å‹ç»Ÿè®¡ã€‘")
    print("\nAIæ–‡æœ¬æ ¼å¼ç±»å‹åˆ†å¸ƒ:")
    for fmt_type, pct in results['ai_stats']['format_type_percentages'].items():
        if pct > 0:
            print(f"  {fmt_type}: {pct:.1f}%")

    print("\näººç±»æ–‡æœ¬æ ¼å¼ç±»å‹åˆ†å¸ƒ:")
    for fmt_type, pct in results['human_stats']['format_type_percentages'].items():
        if pct > 0:
            print(f"  {fmt_type}: {pct:.1f}%")

    # ç®€å•è§„åˆ™æ€§èƒ½
    print(f"\nã€ç®€å•è§„åˆ™æ€§èƒ½ã€‘ï¼ˆä»…åˆ¤æ–­markdownï¼‰")
    print(f"å‡†ç¡®ç‡: {results['simple_rule']['accuracy']*100:.2f}%")
    print(f"ç²¾ç¡®ç‡: {results['simple_rule']['precision']*100:.2f}%")
    print(f"å¬å›ç‡: {results['simple_rule']['recall']*100:.2f}%")
    print(f"F1åˆ†æ•°: {results['simple_rule']['f1']*100:.2f}%")

    cm = results['simple_rule']['confusion_matrix']
    print(f"\næ··æ·†çŸ©é˜µ:")
    print(f"  çœŸæ­£ä¾‹(TP): {cm['tp']} | å‡æ­£ä¾‹(FP): {cm['fp']}")
    print(f"  å‡è´Ÿä¾‹(FN): {cm['fn']} | çœŸè´Ÿä¾‹(TN): {cm['tn']}")

    # è­¦å‘Šä¿¡æ¯
    if results['simple_rule']['accuracy'] > 0.70:
        print(f"\nâš ï¸ è­¦å‘Šï¼šç®€å•è§„åˆ™å‡†ç¡®ç‡ > 70%ï¼Œæ ¼å¼æ˜¯å¼ºä¿¡å·ï¼")
        print(f"   æ¨¡å‹å¯èƒ½ä¸»è¦å­¦ä¹ æ ¼å¼è€Œéè¯­ä¹‰")
        if cm['fn'] > 0:
            print(f"   å‘ç° {cm['fn']} æ¡æ— markdownçš„AIæ–‡æœ¬è¢«è¯¯åˆ¤ä¸ºäººç±»")
    elif results['simple_rule']['accuracy'] < 0.50:
        print(f"\nâœ… ä¼˜ç§€ï¼šç®€å•è§„åˆ™å‡†ç¡®ç‡ < 50%ï¼Œæ ¼å¼å·²ä¸æ˜¯æœ‰æ•ˆç‰¹å¾")


def validate_debiasing_effect(original_results: Dict, debiased_results: Dict):
    """
    éªŒè¯å»åæ•ˆæœ

    Args:
        original_results: åŸå§‹æ•°æ®é›†çš„è¯„ä¼°ç»“æœ
        debiased_results: å»ååæ•°æ®é›†çš„è¯„ä¼°ç»“æœ
    """
    print("\n" + "="*70)
    print("å»åæ•ˆæœéªŒè¯")
    print("="*70)

    # æ ¼å¼åå·®å˜åŒ–
    bias_reduction = original_results['bias'] - debiased_results['bias']
    print(f"\nã€æ ¼å¼åå·®å˜åŒ–ã€‘")
    print(f"åŸå§‹: {original_results['bias']*100:.2f}%")
    print(f"å»åå: {debiased_results['bias']*100:.2f}%")
    print(f"é™ä½: {bias_reduction*100:.2f}% ({'âœ…' if bias_reduction > 0.50 else 'âš ï¸'})")

    # ç®€å•è§„åˆ™å‡†ç¡®ç‡å˜åŒ–
    acc_change = original_results['simple_rule']['accuracy'] - debiased_results['simple_rule']['accuracy']
    print(f"\nã€ç®€å•è§„åˆ™å‡†ç¡®ç‡å˜åŒ–ã€‘")
    print(f"åŸå§‹: {original_results['simple_rule']['accuracy']*100:.2f}%")
    print(f"å»åå: {debiased_results['simple_rule']['accuracy']*100:.2f}%")
    print(f"é™ä½: {acc_change*100:.2f}% ({'âœ…' if acc_change > 0.20 else 'âš ï¸'})")

    # AI markdownç‡å˜åŒ–
    ai_md_reduction = original_results['ai_markdown_rate'] - debiased_results['ai_markdown_rate']
    print(f"\nã€AIæ–‡æœ¬ markdownç‡å˜åŒ–ã€‘")
    print(f"åŸå§‹: {original_results['ai_markdown_rate']*100:.2f}%")
    print(f"å»åå: {debiased_results['ai_markdown_rate']*100:.2f}%")
    print(f"é™ä½: {ai_md_reduction*100:.2f}%")

    # æ€»ä½“è¯„ä¼°
    print(f"\nã€æ€»ä½“è¯„ä¼°ã€‘")
    if debiased_results['bias'] < 0.05 and debiased_results['simple_rule']['accuracy'] < 0.50:
        print("âœ… å»åæ•ˆæœä¼˜ç§€ï¼æ ¼å¼åå·®<5%ï¼Œç®€å•è§„åˆ™å¤±æ•ˆ")
        print("   æ¨¡å‹å°†è¢«è¿«å­¦ä¹ è¯­ä¹‰ç‰¹å¾è€Œéæ ¼å¼ç‰¹å¾")
    elif debiased_results['bias'] < 0.10 and debiased_results['simple_rule']['accuracy'] < 0.60:
        print("âš ï¸ å»åæ•ˆæœè‰¯å¥½ï¼Œä½†ä»æœ‰æ”¹è¿›ç©ºé—´")
    else:
        print("ğŸ”´ å»åæ•ˆæœä¸è¶³ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒæ•´")


def main():
    parser = argparse.ArgumentParser(description='æ ¼å¼åå·®éªŒè¯å·¥å…·')
    parser.add_argument('--original-dir', type=str, default='datasets/bert',
                       help='åŸå§‹æ•°æ®é›†ç›®å½•')
    parser.add_argument('--debiased-dir', type=str, default='datasets/bert_debiased',
                       help='å»ååæ•°æ®é›†ç›®å½•')
    parser.add_argument('--compare', action='store_true',
                       help='å¯¹æ¯”åŸå§‹å’Œå»ååçš„æ•°æ®é›†')
    parser.add_argument('--dataset', type=str, choices=['train', 'val', 'test', 'all'],
                       default='all', help='è¦è¯„ä¼°çš„æ•°æ®é›†')

    args = parser.parse_args()

    print("="*70)
    print("æ ¼å¼åå·®éªŒè¯å·¥å…·")
    print("="*70)
    print(f"å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

    datasets_to_check = ['train', 'val', 'test'] if args.dataset == 'all' else [args.dataset]

    if args.compare:
        # å¯¹æ¯”æ¨¡å¼
        print("æ¨¡å¼ï¼šå¯¹æ¯”åŸå§‹ vs å»åå\n")

        for ds_name in datasets_to_check:
            print(f"\n{'='*70}")
            print(f"è¯„ä¼° {ds_name.upper()} æ•°æ®é›†")
            print(f"{'='*70}")

            # è¯»å–æ•°æ®
            original_df = pd.read_csv(
                f"{args.original_dir}/{ds_name}.csv",
                encoding='utf-8-sig'
            )
            debiased_df = pd.read_csv(
                f"{args.debiased_dir}/{ds_name}.csv",
                encoding='utf-8-sig'
            )

            print(f"\nåŸå§‹æ•°æ®é›†: {len(original_df)}æ¡")
            print(f"å»åæ•°æ®é›†: {len(debiased_df)}æ¡")

            # è¯„ä¼°åŸå§‹æ•°æ®é›†
            print(f"\n[1/2] è¯„ä¼°åŸå§‹æ•°æ®é›†...")
            original_results = evaluate_format_bias(original_df)
            print_format_bias_report(original_results, f"{ds_name.upper()} - åŸå§‹")

            # è¯„ä¼°å»åæ•°æ®é›†
            print(f"\n[2/2] è¯„ä¼°å»åæ•°æ®é›†...")
            debiased_results = evaluate_format_bias(debiased_df)
            print_format_bias_report(debiased_results, f"{ds_name.upper()} - å»åå")

            # éªŒè¯å»åæ•ˆæœ
            validate_debiasing_effect(original_results, debiased_results)

    else:
        # å•ç‹¬è¯„ä¼°æ¨¡å¼
        data_dir = args.debiased_dir if os.path.exists(args.debiased_dir) else args.original_dir
        print(f"æ¨¡å¼ï¼šè¯„ä¼° {data_dir}\n")

        for ds_name in datasets_to_check:
            print(f"\n{'='*70}")
            print(f"è¯„ä¼° {ds_name.upper()} æ•°æ®é›†")
            print(f"{'='*70}")

            # è¯»å–æ•°æ®
            df = pd.read_csv(
                f"{data_dir}/{ds_name}.csv",
                encoding='utf-8-sig'
            )

            print(f"æ ·æœ¬æ•°: {len(df)}æ¡\n")

            # è¯„ä¼°
            results = evaluate_format_bias(df)
            print_format_bias_report(results, ds_name.upper())

    print("\n" + "="*70)
    print("è¯„ä¼°å®Œæˆï¼")
    print("="*70)


if __name__ == "__main__":
    main()
