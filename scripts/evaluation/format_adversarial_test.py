"""
æ ¼å¼å¯¹æŠ—æµ‹è¯•å·¥å…·

åŠŸèƒ½ï¼š
1. æµ‹è¯•æ¨¡å‹å¯¹æ ¼å¼å˜åŒ–çš„é²æ£’æ€§
2. éªŒè¯æ¨¡å‹æ˜¯å¦å­¦ä¹ äº†è¯­ä¹‰è€Œéæ ¼å¼ç‰¹å¾
3. 4ç§å¯¹æŠ—åœºæ™¯å…¨é¢æµ‹è¯•

ä½œè€…ï¼šFormat Debiasing Team
æ—¥æœŸï¼š2026-01-11
"""

import sys
import io
import os
os.environ['PYTHONIOENCODING'] = 'utf-8'

if sys.platform == 'win32':
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
    except:
        pass

import pandas as pd
import torch
import argparse
from datetime import datetime
from typing import Dict, Tuple
from transformers import BertForSequenceClassification, BertTokenizer

# å¯¼å…¥æ ¼å¼å¤„ç†å‡½æ•°
sys.path.insert(0, os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'data_cleaning'))
from format_handler import (
    remove_markdown_comprehensive,
    has_markdown,
    has_markdown_detailed,
    get_format_statistics
)


def evaluate_model(model, df: pd.DataFrame, tokenizer, device='cuda') -> Dict:
    """
    è¯„ä¼°æ¨¡å‹åœ¨æ•°æ®é›†ä¸Šçš„æ€§èƒ½

    Args:
        model: BERTæ¨¡å‹
        df: æµ‹è¯•æ•°æ®æ¡†
        tokenizer: Tokenizer
        device: è®¾å¤‡

    Returns:
        è¯„ä¼°ç»“æœå­—å…¸
    """
    model.eval()
    predictions = []
    labels = df['label'].tolist()

    with torch.no_grad():
        for text in df['text']:
            # ç¡®ä¿textæ˜¯å­—ç¬¦ä¸²ç±»å‹
            if pd.isna(text):
                text = ""
            text = str(text)

            encoding = tokenizer(
                text,
                max_length=512,
                padding='max_length',
                truncation=True,
                return_tensors='pt'
            )

            # ç§»åŠ¨åˆ°è®¾å¤‡
            input_ids = encoding['input_ids'].to(device)
            attention_mask = encoding['attention_mask'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            pred = torch.argmax(outputs.logits, dim=1).item()
            predictions.append(pred)

    # è®¡ç®—å‡†ç¡®ç‡
    correct = sum(1 for p, l in zip(predictions, labels) if p == l)
    accuracy = correct / len(labels)

    # è®¡ç®—æ··æ·†çŸ©é˜µ
    tp = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 1)
    fp = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 0)
    tn = sum(1 for p, l in zip(predictions, labels) if p == 0 and l == 0)
    fn = sum(1 for p, l in zip(predictions, labels) if p == 0 and l == 1)

    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'confusion_matrix': {
            'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn
        },
        'predictions': predictions
    }


def add_markdown_natural(text: str) -> str:
    """
    ä¸ºæ–‡æœ¬è‡ªç„¶åœ°æ·»åŠ markdownæ ¼å¼

    ç­–ç•¥ï¼š
    - 20%æ¦‚ç‡æ·»åŠ æ ‡é¢˜
    - 30%æ¦‚ç‡æ·»åŠ åŠ ç²—
    - 25%æ¦‚ç‡è½¬æ¢ä¸ºåˆ—è¡¨
    - ä¿æŒç®€å•ï¼Œé¿å…è¿‡åº¦æ ¼å¼åŒ–
    """
    import random

    # ç¡®ä¿textæ˜¯å­—ç¬¦ä¸²ç±»å‹
    if pd.isna(text):
        return ""
    text = str(text)

    lines = text.split('\n')
    formatted_lines = []

    for i, line in enumerate(lines):
        line = line.strip()
        if not line:
            formatted_lines.append('')
            continue

        # ç¬¬ä¸€è¡Œæœ‰20%æ¦‚ç‡å˜æˆæ ‡é¢˜
        if i == 0 and random.random() < 0.20:
            line = f"## {line}"

        # çŸ­å¥æœ‰30%æ¦‚ç‡åŠ ç²—å…³é”®è¯
        elif len(line) < 50 and random.random() < 0.30:
            words = line.split()
            if len(words) >= 3:
                # åŠ ç²—å‰å‡ ä¸ªè¯
                words[0] = f"**{words[0]}**"
                line = ' '.join(words)

        # å¦‚æœæ˜¯é€—å·åˆ†éš”çš„å†…å®¹ï¼Œ25%æ¦‚ç‡è½¬ä¸ºåˆ—è¡¨
        elif ',' in line and random.random() < 0.25:
            items = [item.strip() for item in line.split(',') if item.strip()]
            if len(items) >= 2:
                line = '\n'.join(f"- {item}" for item in items)

        formatted_lines.append(line)

    return '\n'.join(formatted_lines)


def format_adversarial_test(model, test_df: pd.DataFrame, tokenizer, device='cuda') -> Dict:
    """
    æ ¼å¼å¯¹æŠ—æµ‹è¯•

    æµ‹è¯•åœºæ™¯ï¼š
    1. å»é™¤æ‰€æœ‰markdownï¼ˆæµ‹è¯•çº¯æ–‡æœ¬æ£€æµ‹ï¼‰
    2. æ·»åŠ æ‰€æœ‰markdownï¼ˆæµ‹è¯•æ ¼å¼å¹²æ‰°ï¼‰
    3. æ ¼å¼äº¤æ¢ï¼ˆAIå»æ ¼å¼ï¼Œäººç±»åŠ æ ¼å¼ï¼‰
    4. éšæœºæ ¼å¼ï¼ˆéšæœºæ·»åŠ /åˆ é™¤ï¼‰

    Args:
        model: BERTæ¨¡å‹
        test_df: æµ‹è¯•æ•°æ®é›†
        tokenizer: Tokenizer
        device: è®¾å¤‡

    Returns:
        å¯¹æŠ—æµ‹è¯•ç»“æœ
    """

    print("="*70)
    print("æ ¼å¼å¯¹æŠ—æµ‹è¯•")
    print("="*70)
    print(f"å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

    # åŸå§‹å‡†ç¡®ç‡
    print("[0/4] è¯„ä¼°åŸå§‹æµ‹è¯•é›†ï¼ˆåŸºçº¿ï¼‰...")
    original_results = evaluate_model(model, test_df, tokenizer, device)
    original_acc = original_results['accuracy']

    print(f"  åŸºçº¿å‡†ç¡®ç‡: {original_acc*100:.2f}%")
    print(f"  åŸºçº¿F1åˆ†æ•°: {original_results['f1']*100:.2f}%")

    # æµ‹è¯•1ï¼šå»é™¤æ‰€æœ‰markdown
    print("\n" + "="*70)
    print("[1/4] æµ‹è¯•1ï¼šå»é™¤æ‰€æœ‰markdownï¼ˆçº¯æ–‡æœ¬æ£€æµ‹ï¼‰")
    print("="*70)
    print("ç›®çš„ï¼šéªŒè¯æ¨¡å‹æ˜¯å¦èƒ½æ£€æµ‹æ— æ ¼å¼çš„AIæ–‡æœ¬\n")

    plain_df = test_df.copy()
    print("  æ­£åœ¨å»é™¤æ‰€æœ‰æ–‡æœ¬çš„markdown...")
    plain_df['text'] = plain_df['text'].apply(remove_markdown_comprehensive)

    # ç»Ÿè®¡æ ¼å¼å»é™¤æ•ˆæœ
    original_md_rate = test_df['text'].apply(has_markdown).mean()
    plain_md_rate = plain_df['text'].apply(has_markdown).mean()
    print(f"  åŸå§‹markdownç‡: {original_md_rate*100:.1f}%")
    print(f"  å»é™¤åmarkdownç‡: {plain_md_rate*100:.1f}%")
    print(f"  æ ¼å¼å»é™¤æ•ˆæœ: {(original_md_rate - plain_md_rate)*100:.1f}%\n")

    print("  è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
    plain_results = evaluate_model(model, plain_df, tokenizer, device)
    plain_acc = plain_results['accuracy']
    plain_drop = plain_acc - original_acc

    print(f"  å‡†ç¡®ç‡: {plain_acc*100:.2f}%")
    print(f"  å˜åŒ–: {plain_drop*100:+.2f}% ({'âœ…' if abs(plain_drop) < 0.05 else 'âš ï¸'})")

    # æµ‹è¯•2ï¼šæ·»åŠ æ‰€æœ‰markdown
    print("\n" + "="*70)
    print("[2/4] æµ‹è¯•2ï¼šä¸ºæ‰€æœ‰æ–‡æœ¬æ·»åŠ markdownï¼ˆæ ¼å¼å¹²æ‰°ï¼‰")
    print("="*70)
    print("ç›®çš„ï¼šéªŒè¯æ ¼å¼ä¸ä¼šè¯¯å¯¼æ¨¡å‹åˆ¤æ–­\n")

    formatted_df = test_df.copy()
    print("  æ­£åœ¨ä¸ºæ‰€æœ‰æ–‡æœ¬æ·»åŠ markdown...")

    # åªä¸ºçº¯æ–‡æœ¬æ·»åŠ markdown
    formatted_df['text'] = formatted_df['text'].apply(
        lambda x: add_markdown_natural(x) if not has_markdown(x) else x
    )

    # ç»Ÿè®¡æ ¼å¼æ·»åŠ æ•ˆæœ
    formatted_md_rate = formatted_df['text'].apply(has_markdown).mean()
    print(f"  åŸå§‹markdownç‡: {original_md_rate*100:.1f}%")
    print(f"  æ·»åŠ åmarkdownç‡: {formatted_md_rate*100:.1f}%")
    print(f"  æ ¼å¼å¢åŠ : {(formatted_md_rate - original_md_rate)*100:+.1f}%\n")

    print("  è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
    formatted_results = evaluate_model(model, formatted_df, tokenizer, device)
    formatted_acc = formatted_results['accuracy']
    formatted_drop = formatted_acc - original_acc

    print(f"  å‡†ç¡®ç‡: {formatted_acc*100:.2f}%")
    print(f"  å˜åŒ–: {formatted_drop*100:+.2f}% ({'âœ…' if abs(formatted_drop) < 0.05 else 'âš ï¸'})")

    # æµ‹è¯•3ï¼šæ ¼å¼äº¤æ¢ï¼ˆæœ€å…³é”®ï¼‰
    print("\n" + "="*70)
    print("[3/4] æµ‹è¯•3ï¼šæ ¼å¼äº¤æ¢ï¼ˆAIâ†’çº¯æ–‡æœ¬ï¼Œäººç±»â†’markdownï¼‰")
    print("="*70)
    print("ç›®çš„ï¼šéªŒè¯æ¨¡å‹ä¸ä¾èµ–æ ¼å¼åˆ¤æ–­ç±»åˆ«\n")

    swapped_df = test_df.copy()
    print("  AIæ–‡æœ¬ï¼šå»é™¤markdown...")
    ai_mask = swapped_df['label'] == 1
    swapped_df.loc[ai_mask, 'text'] = swapped_df.loc[ai_mask, 'text'].apply(
        remove_markdown_comprehensive
    )

    print("  äººç±»æ–‡æœ¬ï¼šæ·»åŠ markdown...")
    human_mask = swapped_df['label'] == 0
    swapped_df.loc[human_mask, 'text'] = swapped_df.loc[human_mask, 'text'].apply(
        add_markdown_natural
    )

    # ç»Ÿè®¡äº¤æ¢æ•ˆæœ
    ai_md_before = test_df[test_df['label']==1]['text'].apply(has_markdown).mean()
    ai_md_after = swapped_df[swapped_df['label']==1]['text'].apply(has_markdown).mean()
    human_md_before = test_df[test_df['label']==0]['text'].apply(has_markdown).mean()
    human_md_after = swapped_df[swapped_df['label']==0]['text'].apply(has_markdown).mean()

    print(f"  AIæ–‡æœ¬markdownç‡: {ai_md_before*100:.1f}% â†’ {ai_md_after*100:.1f}%")
    print(f"  äººç±»æ–‡æœ¬markdownç‡: {human_md_before*100:.1f}% â†’ {human_md_after*100:.1f}%\n")

    print("  è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
    swapped_results = evaluate_model(model, swapped_df, tokenizer, device)
    swapped_acc = swapped_results['accuracy']
    swapped_drop = swapped_acc - original_acc

    print(f"  å‡†ç¡®ç‡: {swapped_acc*100:.2f}%")
    print(f"  å˜åŒ–: {swapped_drop*100:+.2f}% ({'âœ…' if abs(swapped_drop) < 0.10 else 'âš ï¸'})")

    # æµ‹è¯•4ï¼šéšæœºæ ¼å¼
    print("\n" + "="*70)
    print("[4/4] æµ‹è¯•4ï¼šéšæœºæ ¼å¼å˜åŒ–")
    print("="*70)
    print("ç›®çš„ï¼šéªŒè¯æ¨¡å‹åœ¨æ ¼å¼ä¸ç¡®å®šæ—¶çš„ç¨³å®šæ€§\n")

    import random
    random.seed(42)

    random_df = test_df.copy()
    print("  éšæœºå¯¹50%æ–‡æœ¬åº”ç”¨æ ¼å¼å˜åŒ–...")

    for idx in random_df.index:
        if random.random() < 0.5:
            # 50%æ¦‚ç‡å»é™¤æ ¼å¼
            random_df.at[idx, 'text'] = remove_markdown_comprehensive(
                random_df.at[idx, 'text']
            )
        else:
            # 50%æ¦‚ç‡æ·»åŠ æ ¼å¼ï¼ˆå¦‚æœåŸæœ¬æ— æ ¼å¼ï¼‰
            if not has_markdown(random_df.at[idx, 'text']):
                random_df.at[idx, 'text'] = add_markdown_natural(
                    random_df.at[idx, 'text']
                )

    random_md_rate = random_df['text'].apply(has_markdown).mean()
    print(f"  åŸå§‹markdownç‡: {original_md_rate*100:.1f}%")
    print(f"  éšæœºåmarkdownç‡: {random_md_rate*100:.1f}%\n")

    print("  è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
    random_results = evaluate_model(model, random_df, tokenizer, device)
    random_acc = random_results['accuracy']
    random_drop = random_acc - original_acc

    print(f"  å‡†ç¡®ç‡: {random_acc*100:.2f}%")
    print(f"  å˜åŒ–: {random_drop*100:+.2f}% ({'âœ…' if abs(random_drop) < 0.05 else 'âš ï¸'})")

    # æ±‡æ€»ç»“æœ
    print("\n" + "="*70)
    print("æ ¼å¼å…ç–«æ€§è¯„ä¼°")
    print("="*70)

    max_drop = max(
        abs(plain_drop),
        abs(formatted_drop),
        abs(swapped_drop),
        abs(random_drop)
    )

    print(f"\nã€æ€§èƒ½æ±‡æ€»ã€‘")
    print(f"åŸºçº¿å‡†ç¡®ç‡:     {original_acc*100:.2f}%")
    print(f"çº¯æ–‡æœ¬æµ‹è¯•:     {plain_acc*100:.2f}% ({plain_drop*100:+.2f}%)")
    print(f"æ ¼å¼åŒ–æµ‹è¯•:     {formatted_acc*100:.2f}% ({formatted_drop*100:+.2f}%)")
    print(f"æ ¼å¼äº¤æ¢æµ‹è¯•:   {swapped_acc*100:.2f}% ({swapped_drop*100:+.2f}%)")
    print(f"éšæœºæ ¼å¼æµ‹è¯•:   {random_acc*100:.2f}% ({random_drop*100:+.2f}%)")
    print(f"\næœ€å¤§ä¸‹é™å¹…åº¦: {max_drop*100:.2f}%")

    # è¯„çº§
    print("\nã€æ ¼å¼å…ç–«æ€§è¯„çº§ã€‘")
    if max_drop < 0.05:
        grade = "ä¼˜ç§€"
        emoji = "âœ…"
        comment = "æ¨¡å‹å¯¹æ ¼å¼å˜åŒ–å®Œå…¨å…ç–«ï¼ˆä¸‹é™<5%ï¼‰"
        recommendation = "æ¨¡å‹å·²ç»æˆåŠŸå­¦ä¹ è¯­ä¹‰ç‰¹å¾ï¼Œå¯ä»¥éƒ¨ç½²ä½¿ç”¨"
    elif max_drop < 0.10:
        grade = "è‰¯å¥½"
        emoji = "âœ…"
        comment = "æ¨¡å‹å¯¹æ ¼å¼å˜åŒ–è¾ƒä¸ºé²æ£’ï¼ˆä¸‹é™<10%ï¼‰"
        recommendation = "æ¨¡å‹ä¸»è¦åŸºäºè¯­ä¹‰åˆ¤æ–­ï¼Œå¯ä»¥ä½¿ç”¨"
    elif max_drop < 0.20:
        grade = "ä¸­ç­‰"
        emoji = "âš ï¸"
        comment = "æ¨¡å‹ä»ç„¶éƒ¨åˆ†ä¾èµ–æ ¼å¼ç‰¹å¾ï¼ˆä¸‹é™10-20%ï¼‰"
        recommendation = "å»ºè®®è¿›ä¸€æ­¥å¢å¼ºæ•°æ®å»åæˆ–å¢åŠ è®­ç»ƒè½®æ¬¡"
    else:
        grade = "ä¸åˆæ ¼"
        emoji = "ğŸ”´"
        comment = "æ¨¡å‹ä¸¥é‡ä¾èµ–æ ¼å¼ç‰¹å¾ï¼ˆä¸‹é™>20%ï¼‰"
        recommendation = "éœ€è¦é‡æ–°æ£€æŸ¥æ•°æ®é›†æ ¼å¼åˆ†å¸ƒæˆ–è®­ç»ƒç­–ç•¥"

    print(f"{emoji} è¯„çº§: {grade}")
    print(f"   {comment}")
    print(f"\nå»ºè®®: {recommendation}")

    return {
        'original': original_results,
        'plain': plain_results,
        'formatted': formatted_results,
        'swapped': swapped_results,
        'random': random_results,
        'max_drop': max_drop,
        'grade': grade
    }


def main():
    parser = argparse.ArgumentParser(description='æ ¼å¼å¯¹æŠ—æµ‹è¯•å·¥å…·')
    parser.add_argument('--model-dir', type=str, default='models/bert_debiased/best_model',
                       help='æ¨¡å‹ç›®å½•')
    parser.add_argument('--test-file', type=str, default='datasets/bert_debiased/test.csv',
                       help='æµ‹è¯•æ•°æ®æ–‡ä»¶')
    parser.add_argument('--device', type=str, default='cuda',
                       help='è®¾å¤‡ (cuda/cpu)')

    args = parser.parse_args()

    print("="*70)
    print("æ ¼å¼å¯¹æŠ—æµ‹è¯•å·¥å…·")
    print("="*70)
    print(f"å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

    # æ£€æŸ¥è®¾å¤‡
    if args.device == 'cuda' and not torch.cuda.is_available():
        print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°CPU")
        args.device = 'cpu'

    print(f"ä½¿ç”¨è®¾å¤‡: {args.device}\n")

    # åŠ è½½æ¨¡å‹
    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    try:
        model = BertForSequenceClassification.from_pretrained(args.model_dir)
        tokenizer = BertTokenizer.from_pretrained(args.model_dir)
        model.to(args.device)
        model.eval()
        print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ: {args.model_dir}\n")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print(f"   è¯·ç¡®è®¤æ¨¡å‹è·¯å¾„: {args.model_dir}")
        return

    # åŠ è½½æµ‹è¯•æ•°æ®
    print("æ­£åœ¨åŠ è½½æµ‹è¯•æ•°æ®...")
    try:
        test_df = pd.read_csv(args.test_file, encoding='utf-8-sig')
        print(f"âœ“ æµ‹è¯•é›†åŠ è½½æˆåŠŸ: {len(test_df)}æ¡\n")
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return

    # è¿è¡Œå¯¹æŠ—æµ‹è¯•
    results = format_adversarial_test(model, test_df, tokenizer, args.device)

    print("\n" + "="*70)
    print("æµ‹è¯•å®Œæˆï¼")
    print("="*70)


if __name__ == "__main__":
    main()
