#!/usr/bin/env python3
"""
æ•°æ®æ•´åˆè„šæœ¬ - æ•´åˆæ‰€æœ‰é«˜è´¨é‡æ•°æ®æº
"""

import pandas as pd
import numpy as np
import argparse
import os
from pathlib import Path
from sklearn.model_selection import train_test_split
import hashlib

def load_datasets():
    """åŠ è½½æ‰€æœ‰æ•°æ®æº"""
    datasets = []
    
    # 1. å½“å‰å»åæ•°æ®é›†
    try:
        train_df = pd.read_csv('datasets/bert_debiased/train.csv')
        val_df = pd.read_csv('datasets/bert_debiased/val.csv')
        test_df = pd.read_csv('datasets/bert_debiased/test.csv')
        current_df = pd.concat([train_df, val_df, test_df], ignore_index=True)
        current_df['source'] = 'bert_debiased'
        datasets.append(current_df)
        print(f"âœ“ åŠ è½½å»åæ•°æ®é›†: {len(current_df)} æ¡")
    except:
        print("âœ— å»åæ•°æ®é›†æœªæ‰¾åˆ°")
    
    # 2. åŸå§‹AIæ•°æ®
    try:
        ai_df = pd.read_csv('datasets/final/ai_generated_texts.csv')
        ai_df['label'] = 1
        ai_df['source'] = 'ai_generated'
        datasets.append(ai_df)
        print(f"âœ“ åŠ è½½AIæ•°æ®: {len(ai_df)} æ¡")
    except:
        print("âœ— AIæ•°æ®æœªæ‰¾åˆ°")
    
    # 3. äººç±»æ•°æ®
    try:
        human_df = pd.read_csv('datasets/human_texts/human_texts.csv')
        human_df['label'] = 0
        human_df['source'] = 'human_texts'
        datasets.append(human_df)
        print(f"âœ“ åŠ è½½äººç±»æ•°æ®: {len(human_df)} æ¡")
    except:
        print("âœ— äººç±»æ•°æ®æœªæ‰¾åˆ°")
    
    # 4. æ ‡å‡†æ•°æ®é›†
    for dataset_name in ['hc3', 'thucnews', 'mgtbench']:
        try:
            std_df = pd.read_csv(f'datasets/standard/{dataset_name}_processed.csv')
            std_df['source'] = dataset_name
            datasets.append(std_df)
            print(f"âœ“ åŠ è½½{dataset_name}: {len(std_df)} æ¡")
        except:
            print(f"âœ— {dataset_name}æ•°æ®æœªæ‰¾åˆ°")
    
    return datasets

def standardize_format(datasets):
    """ç»Ÿä¸€æ•°æ®æ ¼å¼"""
    standardized = []
    
    for df in datasets:
        # ç¡®ä¿å¿…è¦åˆ—å­˜åœ¨
        if 'text' not in df.columns:
            continue
        
        # æ ‡å‡†åŒ–åˆ—å
        std_df = pd.DataFrame()
        std_df['text'] = df['text']
        std_df['label'] = df.get('label', 0)
        std_df['source'] = df.get('source', 'unknown')
        
        # è®¡ç®—é•¿åº¦
        std_df['length'] = std_df['text'].str.len()
        
        # æ·»åŠ æ–‡æœ¬å“ˆå¸Œç”¨äºå»é‡
        std_df['text_hash'] = std_df['text'].apply(
            lambda x: hashlib.md5(str(x).encode()).hexdigest()
        )
        
        standardized.append(std_df)
    
    return pd.concat(standardized, ignore_index=True)

def remove_duplicates(df):
    """å»é‡"""
    print(f"å»é‡å‰: {len(df)} æ¡")
    df_dedup = df.drop_duplicates(subset=['text_hash'], keep='first')
    print(f"å»é‡å: {len(df_dedup)} æ¡ (ç§»é™¤ {len(df) - len(df_dedup)} æ¡)")
    return df_dedup.drop('text_hash', axis=1)

def filter_quality(df):
    """è¿‡æ»¤ä½è´¨é‡æ•°æ®"""
    print(f"è´¨é‡è¿‡æ»¤å‰: {len(df)} æ¡")
    
    # è¿‡æ»¤æ¡ä»¶
    mask = (
        (df['length'] >= 50) &  # æœ€å°‘50å­—ç¬¦
        (df['length'] <= 5000) &  # æœ€å¤š5000å­—ç¬¦
        (df['text'].str.strip() != '') &  # éç©º
        (~df['text'].str.contains(r'^[\s\n\r]*$', regex=True))  # éçº¯ç©ºç™½
    )
    
    df_filtered = df[mask].copy()
    print(f"è´¨é‡è¿‡æ»¤å: {len(df_filtered)} æ¡ (ç§»é™¤ {len(df) - len(df_filtered)} æ¡)")
    return df_filtered

def balance_length(df, target_samples=20000):
    """é•¿åº¦å¹³è¡¡ - åˆ†å±‚é‡‡æ ·"""
    print(f"é•¿åº¦å¹³è¡¡å‰: {len(df)} æ¡")
    
    # å®šä¹‰é•¿åº¦åŒºé—´
    df['length_bin'] = pd.cut(df['length'], 
                             bins=[0, 200, 500, 1000, 2000, float('inf')], 
                             labels=['very_short', 'short', 'medium', 'long', 'very_long'])
    
    balanced_dfs = []
    
    for label in [0, 1]:
        label_df = df[df['label'] == label].copy()
        
        # æ¯ä¸ªæ ‡ç­¾ç›®æ ‡æ ·æœ¬æ•°
        label_target = target_samples // 2
        
        # æŒ‰é•¿åº¦åŒºé—´åˆ†å±‚é‡‡æ ·
        sampled_parts = []
        for bin_name in ['very_short', 'short', 'medium', 'long', 'very_long']:
            bin_df = label_df[label_df['length_bin'] == bin_name]
            if len(bin_df) > 0:
                # æ¯ä¸ªåŒºé—´é‡‡æ ·ç›¸åŒæ¯”ä¾‹
                bin_target = min(len(bin_df), label_target // 5)
                if len(bin_df) > bin_target:
                    bin_sampled = bin_df.sample(n=bin_target, random_state=42)
                else:
                    bin_sampled = bin_df
                sampled_parts.append(bin_sampled)
        
        if sampled_parts:
            label_balanced = pd.concat(sampled_parts, ignore_index=True)
            balanced_dfs.append(label_balanced)
    
    result = pd.concat(balanced_dfs, ignore_index=True).drop('length_bin', axis=1)
    print(f"é•¿åº¦å¹³è¡¡å: {len(result)} æ¡")
    
    # æ‰“å°é•¿åº¦åˆ†å¸ƒ
    print("\né•¿åº¦åˆ†å¸ƒ:")
    for label in [0, 1]:
        label_data = result[result['label'] == label]
        print(f"æ ‡ç­¾{label}: å¹³å‡é•¿åº¦ {label_data['length'].mean():.0f}, "
              f"ä¸­ä½æ•° {label_data['length'].median():.0f}")
    
    return result

def split_dataset(df, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):
    """åˆ’åˆ†æ•°æ®é›†"""
    # åˆ†å±‚åˆ’åˆ†ä¿æŒæ ‡ç­¾å¹³è¡¡
    train_df, temp_df = train_test_split(
        df, test_size=(val_ratio + test_ratio), 
        stratify=df['label'], random_state=42
    )
    
    val_df, test_df = train_test_split(
        temp_df, test_size=test_ratio/(val_ratio + test_ratio),
        stratify=temp_df['label'], random_state=42
    )
    
    print(f"\næ•°æ®é›†åˆ’åˆ†:")
    print(f"è®­ç»ƒé›†: {len(train_df)} æ¡ ({len(train_df)/len(df)*100:.1f}%)")
    print(f"éªŒè¯é›†: {len(val_df)} æ¡ ({len(val_df)/len(df)*100:.1f}%)")
    print(f"æµ‹è¯•é›†: {len(test_df)} æ¡ ({len(test_df)/len(df)*100:.1f}%)")
    
    return train_df, val_df, test_df

def save_datasets(train_df, val_df, test_df, output_dir):
    """ä¿å­˜æ•°æ®é›†"""
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜åˆ†å‰²æ•°æ®
    train_df.to_csv(output_path / 'train.csv', index=False)
    val_df.to_csv(output_path / 'val.csv', index=False)
    test_df.to_csv(output_path / 'test.csv', index=False)
    
    # ä¿å­˜å®Œæ•´æ•°æ®
    full_df = pd.concat([train_df, val_df, test_df], ignore_index=True)
    full_df.to_csv(output_path / 'full_dataset.csv', index=False)
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_samples': len(full_df),
        'train_samples': len(train_df),
        'val_samples': len(val_df),
        'test_samples': len(test_df),
        'ai_samples': len(full_df[full_df['label'] == 1]),
        'human_samples': len(full_df[full_df['label'] == 0]),
        'sources': full_df['source'].value_counts().to_dict(),
        'avg_length_ai': full_df[full_df['label'] == 1]['length'].mean(),
        'avg_length_human': full_df[full_df['label'] == 0]['length'].mean()
    }
    
    with open(output_path / 'dataset_stats.txt', 'w', encoding='utf-8') as f:
        f.write("æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯\n")
        f.write("=" * 50 + "\n")
        for key, value in stats.items():
            f.write(f"{key}: {value}\n")
    
    print(f"\nâœ“ æ•°æ®é›†å·²ä¿å­˜åˆ°: {output_path}")
    print(f"âœ“ ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {output_path / 'dataset_stats.txt'}")

def main():
    parser = argparse.ArgumentParser(description='æ•´åˆæ‰€æœ‰æ•°æ®é›†')
    parser.add_argument('--output', '-o', default='datasets/integrated',
                       help='è¾“å‡ºç›®å½• (é»˜è®¤: datasets/integrated)')
    parser.add_argument('--samples', '-s', type=int, default=20000,
                       help='ç›®æ ‡æ ·æœ¬æ•° (é»˜è®¤: 20000)')
    
    args = parser.parse_args()
    
    print("å¼€å§‹æ•°æ®æ•´åˆ...")
    
    # 1. åŠ è½½æ•°æ®
    datasets = load_datasets()
    if not datasets:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ•°æ®é›†")
        return
    
    # 2. ç»Ÿä¸€æ ¼å¼
    df = standardize_format(datasets)
    print(f"\nç»Ÿä¸€æ ¼å¼å: {len(df)} æ¡")
    
    # 3. å»é‡
    df = remove_duplicates(df)
    
    # 4. è´¨é‡è¿‡æ»¤
    df = filter_quality(df)
    
    # 5. é•¿åº¦å¹³è¡¡
    df = balance_length(df, args.samples)
    
    # 6. åˆ’åˆ†æ•°æ®é›†
    train_df, val_df, test_df = split_dataset(df)
    
    # 7. ä¿å­˜
    save_datasets(train_df, val_df, test_df, args.output)
    
    print("\nğŸ‰ æ•°æ®æ•´åˆå®Œæˆ!")

if __name__ == '__main__':
    main()